{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Mask R-CNN - Train FCN using MRCNN in Predict Mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:49:50.082013Z",
     "start_time": "2018-07-09T15:49:49.100851Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_in_batches_dev(self,\n",
    "          mrcnn_model,\n",
    "          train_dataset, \n",
    "          val_dataset, \n",
    "          learning_rate, \n",
    "          layers            = None,\n",
    "          losses            = None,              \n",
    "          epochs            = 0,\n",
    "          epochs_to_run     = 1, \n",
    "          batch_size        = 0, \n",
    "          steps_per_epoch   = 0,\n",
    "          min_LR            = 0.00001):\n",
    "\n",
    "    '''\n",
    "    Train the model.\n",
    "    train_dataset, \n",
    "    val_dataset:    Training and validation Dataset objects.\n",
    "\n",
    "    learning_rate:  The learning rate to train with\n",
    "\n",
    "    epochs:         Number of training epochs. Note that previous training epochs\n",
    "                    are considered to be done already, so this actually determines\n",
    "                    the epochs to train in total rather than in this particaular\n",
    "                    call.\n",
    "\n",
    "    layers:         Allows selecting wich layers to train. It can be:\n",
    "                    - A regular expression to match layer names to train\n",
    "                    - One of these predefined values:\n",
    "                    heads: The RPN, classifier and mask heads of the network\n",
    "                    all: All the layers\n",
    "                    3+: Train Resnet stage 3 and up\n",
    "                    4+: Train Resnet stage 4 and up\n",
    "                    5+: Train Resnet stage 5 and up\n",
    "    '''\n",
    "    assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "\n",
    "    if batch_size == 0 :\n",
    "        batch_size = self.config.BATCH_SIZE\n",
    "\n",
    "    # if epochs_to_run > 0 :\n",
    "    epochs = self.epoch + epochs_to_run\n",
    "\n",
    "    if steps_per_epoch == 0:\n",
    "        steps_per_epoch = self.config.STEPS_PER_EPOCH\n",
    "\n",
    "    # use Pre-defined layer regular expressions\n",
    "    # if layers in self.layer_regex.keys():\n",
    "        # layers = self.layer_regex[layers]\n",
    "    print(layers)\n",
    "    # train_regex_list = []\n",
    "    # for x in layers:\n",
    "        # print( ' layers ias : ',x)\n",
    "        # train_regex_list.append(x)\n",
    "    train_regex_list = [self.layer_regex[x] for x in layers]\n",
    "    print(train_regex_list)\n",
    "    layers = '|'.join(train_regex_list)        \n",
    "    print('layers regex :', layers)\n",
    "\n",
    "\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    ## Data generators\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    train_generator = data_generator(train_dataset, self.config, shuffle=True,\n",
    "                                     batch_size=batch_size)\n",
    "    val_generator   = data_generator(val_dataset, self.config, shuffle=True,\n",
    "                                     batch_size=batch_size,\n",
    "                                     augment=False)\n",
    "\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    ## Set trainable layers and compile\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    self.set_trainable(layers)            \n",
    "    self.compile(learning_rate, self.config.LEARNING_MOMENTUM, losses)        \n",
    "\n",
    "\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    ## Create checkpoint folder if it doesn't exists\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    from tensorflow.python.platform import gfile\n",
    "    if not gfile.IsDirectory(self.log_dir):\n",
    "        log('Creating checkpoint folder : {}'.format(self.log_dir))\n",
    "        gfile.MakeDirs(self.log_dir)\n",
    "    else:\n",
    "        log('Checkpoint folder already exists: {}'.format(self.log_dir))                                   \n",
    "    # my_callback = MyCallback()\n",
    "\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    ## Callbacks\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    # call back for model checkpoint was originally (?) loss. chanegd to val_loss (which is default) 2-5-18\n",
    "\n",
    "    # copied from \\keras\\engine\\training.py\n",
    "    # def _get_deduped_metrics_names(self):\n",
    "    ## get metrics from keras_model.metrics_names\n",
    "    out_labels = self.get_deduped_metrics_names()\n",
    "    print()\n",
    "    print(' out_labels from get_deduped_metrics_names() : ')\n",
    "    print(' --------------------------------------------- ')\n",
    "    print(out_labels)\n",
    "\n",
    "    ## setup Progress Bar callback\n",
    "    callback_metrics = out_labels + ['val_' + n for n in out_labels]\n",
    "    print()\n",
    "    print(' Callback metrics monitored by progbar :')\n",
    "    print(' ---------------------------------------')\n",
    "    pp.pprint(callback_metrics)\n",
    "\n",
    "    # progbar = keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "    # progbar.set_model(self.keras_model)\n",
    "    # progbar.set_params({\n",
    "        # 'epochs': epochs,\n",
    "        # 'steps': steps_per_epoch,\n",
    "        # 'verbose': 1,\n",
    "        # 'do_validation': False,\n",
    "        # 'metrics': callback_metrics,\n",
    "    # })\n",
    "\n",
    "\n",
    "    # progbar.set_model(self.keras_model) \n",
    "\n",
    "    ## setup Checkpoint callback\n",
    "    # chkpoint = keras.callbacks.ModelCheckpoint(self.checkpoint_path, \n",
    "                                               # monitor='val_loss', \n",
    "                                               # verbose=1, \n",
    "                                               # save_best_only = True, \n",
    "                                               # save_weights_only=True)\n",
    "    # chkpoint.set_model(self.keras_model)\n",
    "\n",
    "    # progbar.on_train_begin()\n",
    "\n",
    "\n",
    "\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.ProgbarLogger(count_mode='steps'),\n",
    "\n",
    "        keras.callbacks.TensorBoard(log_dir=self.log_dir,\n",
    "                                      histogram_freq=0,\n",
    "                                      batch_size=32,\n",
    "                                      write_graph=True,\n",
    "                                      write_grads=False,\n",
    "                                      write_images=True,\n",
    "                                      embeddings_freq=0,\n",
    "                                      embeddings_layer_names=None,\n",
    "                                      embeddings_metadata=None)\n",
    "\n",
    "        , keras.callbacks.ModelCheckpoint(self.checkpoint_path, \n",
    "                                          mode = 'auto', \n",
    "                                          period = 1, \n",
    "                                          monitor='val_loss', \n",
    "                                          verbose=1, \n",
    "                                          save_best_only = True, \n",
    "                                          save_weights_only=True)\n",
    "\n",
    "        , keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            mode     = 'auto', \n",
    "                                            factor   = self.config.REDUCE_LR_FACTOR,   \n",
    "                                            cooldown = self.config.REDUCE_LR_COOLDOWN,\n",
    "                                            patience = self.config.REDUCE_LR_PATIENCE,\n",
    "                                            min_lr   = self.config.MIN_LR, \n",
    "                                            verbose  = 1)                                            \n",
    "\n",
    "        , keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            mode      = 'auto', \n",
    "                                            min_delta = 0.00001, \n",
    "                                            patience  = self.config.EARLY_STOP_PATIENCE, \n",
    "                                            verbose   = 1)                                            \n",
    "        # , my_callback\n",
    "    ]\n",
    "\n",
    "\n",
    "    callbacks =  keras.callbacks.CallbackList(callbacks = callbacks_list)\n",
    "    callbacks.set_model(self.keras_model)\n",
    "    callbacks.set_params({\n",
    "        'epochs': epochs,\n",
    "        'steps': steps_per_epoch,\n",
    "        'verbose': 1,\n",
    "        'do_validation': False,\n",
    "        'metrics': callback_metrics,\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    log(\"Starting at epoch {} of {} epochs. LR={}\\n\".format(self.epoch, epochs, learning_rate))\n",
    "    log(\"Steps per epochs {} \".format(steps_per_epoch))\n",
    "    log(\"    Last epoch completed : {} \".format(self.epoch))\n",
    "    log(\"    Starting from epoch  : {} for {} epochs\".format(self.epoch, epochs_to_run))\n",
    "    log(\"    Learning Rate        : {} \".format(learning_rate))\n",
    "    log(\"    Steps per epoch      : {} \".format(steps_per_epoch))\n",
    "    log(\"    Batch Size           : {} \".format(batch_size))\n",
    "    log(\"    Checkpoint Folder    : {} \".format(self.checkpoint_path))\n",
    "\n",
    "\n",
    "\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    ## Start main training loop\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    epoch_idx = self.epoch\n",
    "    # progbar.on_train_begin()\n",
    "    callbacks.on_train_begin()\n",
    "\n",
    "    if epoch_idx >= epochs:\n",
    "        print('Final epoch {} has already completed - Training will not proceed'.format(epochs))\n",
    "    else:\n",
    "\n",
    "        while epoch_idx < epochs :\n",
    "            # progbar.on_epoch_begin(epoch_idx)\n",
    "            callbacks.on_epoch_begin(epoch_idx)\n",
    "\n",
    "            for steps_index in range(steps_per_epoch):\n",
    "\n",
    "                batch_logs = {}\n",
    "                print(' self.epoch {}   epochs {}  step {} '.format(self.epoch, epochs, steps_index))\n",
    "                batch_logs['batch'] = steps_index\n",
    "                batch_logs['size']  = batch_size\n",
    "                # progbar.on_batch_begin(steps_index, batch_logs)\n",
    "                callbacks.on_batch_begin(steps_index, batch_logs)\n",
    "\n",
    "                train_batch_x, train_batch_y = next(train_generator)\n",
    "                # print('length of train_batch_x:', len(train_batch_x))\n",
    "                # print('length of train_batch_y:', len(train_batch_y))\n",
    "\n",
    "\n",
    "                # # model_output   = get_layer_output_2(mrcnn_model.keras_model, train_batch_x, training_flag = False)\n",
    "                # # model_output = get_layer_output_1(model.keras_model, train_batch_x, [ 26], 1)\n",
    "\n",
    "                # print(len(model_output))\n",
    "                # # print(type(output_rois))\n",
    "                # for i in model_output:\n",
    "                    # print( i.shape)                    \n",
    "\n",
    "                results = mrcnn_model.keras_model.predict(train_batch_x)\n",
    "#                 print('# of items in results:', len(results))\n",
    "\n",
    "\n",
    "                pr_hm_norm, gt_hm_norm, pr_hm_scores, gt_hm_scores = results[11:]                 \n",
    "\n",
    "                # print('pr_hm_norm shape   :', pr_hm_norm.shape)\n",
    "                # print('pr_hm_scores shape :', pr_hm_scores.shape)\n",
    "                # print('gt_hm_norm shape   :', gt_hm_norm.shape)\n",
    "                # print('gt_hm_scores shape :', gt_hm_scores.shape)\n",
    "\n",
    "                outs = self.keras_model.train_on_batch([pr_hm_norm,  pr_hm_scores,gt_hm_norm, gt_hm_scores], train_batch_y)\n",
    "\n",
    "#                 print(' outs: ', outs)\n",
    "                if not isinstance(outs, list):\n",
    "                    outs = [outs]\n",
    "                for l, o in zip(out_labels, outs):\n",
    "                    batch_logs[l] = o\n",
    "\n",
    "                # progbar.on_batch_end(steps_index, batch_logs)\n",
    "                callbacks.on_batch_end(steps_index, batch_logs)\n",
    "\n",
    "                # print(outs)\n",
    "\n",
    "            ## end of epoch operations     \n",
    "            ##-------------------------------\n",
    "            val_batch_x, val_batch_y = next(val_generator)\n",
    "            val_outs = self.keras_model.test_on_batch(X_val, Y_val)\n",
    "            # write_log(callback, val_names, logs, batch_no//10)\n",
    "            print(' validation logs output: ', val_outs)\n",
    "            if not isinstance(val_outs, list):\n",
    "                val_outs = [val_outs]\n",
    "            for l, o in zip(out_labels, outs):\n",
    "                batch_logs[l] = o\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # progbar.on_epoch_end(epoch_idx, {})\n",
    "            # if (epoch_idx % 10) == 0:\n",
    "            # chkpoint.on_epoch_end(epoch_idx  , batch_logs)\n",
    "            callbacks.on_epoch_end(epoch_idx, batch_logs)\n",
    "            epoch_idx += 1\n",
    "\n",
    "        ## end of all epochs operations\n",
    "        ##--------------------------------\n",
    "        # if epoch_idx != self.epoch:\n",
    "        # chkpoint.on_epoch_end(epoch_idx -1, batch_logs)\n",
    "        callbacks.on_train_end()\n",
    "        self.epoch = max(epoch_idx - 1, epochs)\n",
    "        print('Final : self.epoch {}   epochs {}'.format(self.epoch, epochs))\n",
    "\n",
    "    ##--------------------------------------------------------------------------------\n",
    "    ## End main training loop\n",
    "    ##--------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:38:11.488298Z",
     "start_time": "2018-07-09T15:40:38.570Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_layers)\n",
    "# train_regex_list = []\n",
    "# for x in layers:\n",
    "    # print( ' layers ias : ',x)\n",
    "    # train_regex_list.append(x)\n",
    "train_regex_list = [fcn_model.layer_regex[x] for x in train_layers]\n",
    "print(train_regex_list)\n",
    "layers = '|'.join(train_regex_list)        \n",
    "print('layers regex :', layers)\n",
    "\n",
    "##--------------------------------------------------------------------------------\n",
    "## Set trainable layers and compile\n",
    "##--------------------------------------------------------------------------------\n",
    "fcn_model.set_trainable(layers)            \n",
    "fcn_model.compile(learning_rate, fcn_model.config.LEARNING_MOMENTUM, loss_names)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:13:22.946697Z",
     "start_time": "2018-07-09T15:13:22.906821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " out_labels from get_deduped_metrics_names() : \n",
      " --------------------------------------------- \n",
      "['loss', 'fcn_norm_loss']\n"
     ]
    }
   ],
   "source": [
    "out_labels = fcn_model.get_deduped_metrics_names()\n",
    "print()\n",
    "print(' out_labels from get_deduped_metrics_names() : ')\n",
    "print(' --------------------------------------------- ')\n",
    "print(out_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:09:27.449382Z",
     "start_time": "2018-07-09T15:09:27.396705Z"
    }
   },
   "outputs": [],
   "source": [
    "##--------------------------------------------------------------------------------\n",
    "## Data generators\n",
    "##--------------------------------------------------------------------------------\n",
    "train_generator = data_generator(dataset_train, mrcnn_model.config, shuffle=True,\n",
    "                                 batch_size=batch_size)\n",
    "val_generator   = data_generator(dataset_val, mrcnn_model.config, shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_config.EPOCHS_TO_RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:09:30.828699Z",
     "start_time": "2018-07-09T15:09:30.786146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at epoch 0 of 100 epochs. LR=0.005\n",
      "\n",
      "Steps per epochs 128 \n",
      "    Last epoch completed : 0 \n",
      "    Starting from epoch  : 0 for 100 epochs\n",
      "    Learning Rate        : 0.005 \n",
      "    Steps per epoch      : 128 \n",
      "    Batch Size           : 8 \n",
      "    Checkpoint Folder    : /home/kbardool/models/train_fcn_alt/shapes20180709T1458/fcn_shapes_{epoch:04d}.h5 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = fcn_model.epoch + epochs_to_run\n",
    "log(\"Starting at epoch {} of {} epochs. LR={}\\n\".format(fcn_model.epoch, epochs, learning_rate))\n",
    "log(\"Steps per epochs {} \".format(steps_per_epoch))\n",
    "log(\"    Last epoch completed : {} \".format(fcn_model.epoch))\n",
    "log(\"    Starting from epoch  : {} for {} epochs\".format(fcn_model.epoch, epochs_to_run))\n",
    "log(\"    Learning Rate        : {} \".format(learning_rate))\n",
    "log(\"    Steps per epoch      : {} \".format(steps_per_epoch))\n",
    "log(\"    Batch Size           : {} \".format(batch_size))\n",
    "log(\"    Checkpoint Folder    : {} \".format(fcn_model.checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:15:38.298611Z",
     "start_time": "2018-07-09T15:15:38.262121Z"
    }
   },
   "outputs": [],
   "source": [
    "steps_index = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:15:39.537649Z",
     "start_time": "2018-07-09T15:15:39.488679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " self.epoch 0   epochs 100  step 0 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'callbacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d49075fba920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# progbar.on_batch_begin(steps_index, batch_logs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'callbacks' is not defined"
     ]
    }
   ],
   "source": [
    "batch_logs = {}\n",
    "print(' self.epoch {}   epochs {}  step {} '.format(fcn_model.epoch, epochs, steps_index))\n",
    "batch_logs['batch'] = steps_index\n",
    "batch_logs['size']  = batch_size\n",
    "# progbar.on_batch_begin(steps_index, batch_logs)\n",
    "callbacks.on_batch_begin(steps_index, batch_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:09:35.323441Z",
     "start_time": "2018-07-09T15:09:35.225145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128, 128, 3)\n",
      "(8, 15)\n",
      "(8, 4092, 1)\n",
      "(8, 256, 4)\n",
      "(8, 100)\n",
      "(8, 100, 4)\n",
      "(8, 56, 56, 100)\n"
     ]
    }
   ],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)\n",
    "\n",
    "for i in train_batch_x:\n",
    "    print( i.shape)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MRCNN model predictions\n",
    "\n",
    "     Outputs:\n",
    "     --------\n",
    "     layer:  0    output : rpn_class_logits/rpn_class_logits:0        Type: float32           Shape: (?, ?, 2)\n",
    "     layer:  1    output : rpn_class/rpn_class:0                      Type: float32           Shape: (?, ?, 2)\n",
    "     layer:  2    output : rpn_bbox/rpn_bbox:0                        Type: float32           Shape: (?, ?, 4)\n",
    "     layer:  3    output : rpn_proposal_rois/rpn_roi_proposals:0      Type: float32           Shape: (8, ?, ?)\n",
    "     layer:  4    output : proposal_targets/output_rois:0             Type: float32           Shape: (8, ?, ?)\n",
    "     layer:  5    output : proposal_targets/target_class_ids:0        Type: int32             Shape: (8, ?)\n",
    "     layer:  6    output : proposal_targets/target_bbox_deltas:0      Type: float32           Shape: (8, ?, ?)\n",
    "     layer:  7    output : proposal_targets/roi_gt_boxes:0            Type: float32           Shape: (8, ?, ?)\n",
    "     layer:  8    output : mrcnn_class_logits/mrcnn_class_logits:0    Type: float32           Shape: (?, 32, 7)\n",
    "     layer:  9    output : mrcnn_class/mrcnn_class:0                  Type: float32           Shape: (?, 32, 7)\n",
    "     layer: 10    output : mrcnn_bbox_regression/mrcnn_bbox:0         Type: float32           Shape: (?, 32, 7, 4)\n",
    "     layer: 11    output : cntxt_layer/pred_heatmap_norm:0            Type: float32           Shape: (8, 128, 128, 7)\n",
    "     layer: 12    output : cntxt_layer/gt_heatmap_norm:0              Type: float32           Shape: (8, 128, 128, 7)\n",
    "     layer: 13    output : cntxt_layer/pred_heatmap_scores:0          Type: float32           Shape: (?, ?, ?, ?)\n",
    "     layer: 14    output : cntxt_layer/gt_heatmap_scores:0            Type: float32           Shape: (?, ?, ?, ?)\n",
    "        \n",
    "\n",
    "     Inputs:\n",
    "     -------\n",
    "     layer:  0    output : input_pr_hm_norm:0                         Type: float32           Shape: (?, 128, 128, 7)\n",
    "     layer:  1    output : input_pr_hm_scores:0                       Type: float32           Shape: (?, 7, 32, 11)\n",
    "     layer:  2    output : input_gt_hm_norm:0                         Type: float32           Shape: (?, 128, 128, 7)\n",
    "     layer:  3    output : input_gt_hm_scores:0                       Type: float32           Shape: (?, 7, 32, 11)\n",
    "\n",
    "\n",
    "     Outputs:\n",
    "     --------\n",
    "     layer:  0    output : fcn_heatmap_norm/fcn_heatmap_norm:0        Type: float32           Shape: (?, 128, 128, 7)\n",
    "     layer:  1    output : fcn_scoring/fcn_heatmap_scores:0           Type: float32           Shape: (?, ?, ?, ?)\n",
    "     layer:  2    output : fcn_heatmap/fcn_heatmap:0                  Type: float32           Shape: (?, 128, 128, 7)\n",
    "     layer:  3    output : fcn_norm_loss/fcn_norm_loss:0              Type: float32           Shape: (1, 1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:09:44.907473Z",
     "start_time": "2018-07-09T15:09:40.245616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items in results: 15\n",
      "(8, 4092, 2)\n",
      "(8, 4092, 2)\n",
      "(8, 4092, 4)\n",
      "(8, 1000, 4)\n",
      "(8, 32, 4)\n",
      "(8, 32)\n",
      "(8, 32, 4)\n",
      "(8, 32, 4)\n",
      "(8, 32, 7)\n",
      "(8, 32, 7)\n",
      "(8, 32, 7, 4)\n",
      "(8, 128, 128, 7)\n",
      "(8, 128, 128, 7)\n",
      "(8, 7, 32, 11)\n",
      "(8, 7, 32, 11)\n"
     ]
    }
   ],
   "source": [
    "results = mrcnn_model.keras_model.predict(train_batch_x)\n",
    "print('# of items in results:', len(results))\n",
    "\n",
    "for i in results:\n",
    "    print( i.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:09:49.417601Z",
     "start_time": "2018-07-09T15:09:49.377184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128, 128, 7)\n",
      "(8, 128, 128, 7)\n",
      "(8, 7, 32, 11)\n",
      "(8, 7, 32, 11)\n"
     ]
    }
   ],
   "source": [
    "pr_hm_norm, gt_hm_norm, pr_hm_scores, gt_hm_scores = results[11:]                 \n",
    "print(pr_hm_norm.shape)\n",
    "print(gt_hm_norm.shape)\n",
    "print(pr_hm_scores.shape)\n",
    "print(gt_hm_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:10:05.116891Z",
     "start_time": "2018-07-09T15:09:55.156786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Make training function -------------------------------\n",
      " self.total_loss : loss_1/add_1:0\n",
      "<tf.Tensor 'loss_1/add_1:0' shape=(1, 1) dtype=float32>\n",
      " self.metrics_tensors : Mean_3:0\n",
      "[<tf.Tensor 'Mean_3:0' shape=(1, 1) dtype=float32>]\n",
      " updates : \n",
      "[   <tf.Tensor 'training/SGD/AssignAdd:0' shape=() dtype=int64_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign:0' shape=(3, 3, 7, 64) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_1:0' shape=(3, 3, 7, 64) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_2:0' shape=(64,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_3:0' shape=(64,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_4:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_5:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_6:0' shape=(64,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_7:0' shape=(64,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_8:0' shape=(3, 3, 64, 128) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_9:0' shape=(3, 3, 64, 128) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_10:0' shape=(128,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_11:0' shape=(128,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_12:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_13:0' shape=(3, 3, 128, 128) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_14:0' shape=(128,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_15:0' shape=(128,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_16:0' shape=(3, 3, 128, 256) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_17:0' shape=(3, 3, 128, 256) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_18:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_19:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_20:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_21:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_22:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_23:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_24:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_25:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_26:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_27:0' shape=(256,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_28:0' shape=(3, 3, 256, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_29:0' shape=(3, 3, 256, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_30:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_31:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_32:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_33:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_34:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_35:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_36:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_37:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_38:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_39:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_40:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_41:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_42:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_43:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_44:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_45:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_46:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_47:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_48:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_49:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_50:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_51:0' shape=(512,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_52:0' shape=(7, 7, 512, 2048) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_53:0' shape=(7, 7, 512, 2048) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_54:0' shape=(2048,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_55:0' shape=(2048,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_56:0' shape=(1, 1, 2048, 2048) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_57:0' shape=(1, 1, 2048, 2048) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_58:0' shape=(2048,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_59:0' shape=(2048,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_60:0' shape=(1, 1, 2048, 7) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_61:0' shape=(1, 1, 2048, 7) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_62:0' shape=(7,) dtype=float32_ref>,\n",
      "    <tf.Tensor 'training/SGD/Assign_63:0' shape=(7,) dtype=float32_ref>]\n",
      "  ------------------------------------------------------\n",
      "train_on_batch(1929) outputs:  <class 'list'>  len:  2\n",
      "   type: <class 'numpy.ndarray'>   shape:  (1, 1) [[0.0041]]\n",
      "   type: <class 'numpy.ndarray'>   shape:  (1, 1) [[0.0041]]\n"
     ]
    }
   ],
   "source": [
    "outs = fcn_model.keras_model.train_on_batch([pr_hm_norm,  pr_hm_scores,gt_hm_norm, gt_hm_scores], train_batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:13:32.839825Z",
     "start_time": "2018-07-09T15:13:32.787299Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.0041]], dtype=float32), array([[0.0041]], dtype=float32)]\n",
      " outs:  [array([[0.0041]], dtype=float32), array([[0.0041]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4f1a8c148282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_logs' is not defined"
     ]
    }
   ],
   "source": [
    "pp.pprint(outs)\n",
    "print(' outs: ', outs)\n",
    "if not isinstance(outs, list):\n",
    "    outs = [outs]\n",
    "for l, o in zip(out_labels, outs):\n",
    "    batch_logs[l] = o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_output = get_layer_output_2(fcn_model.keras_model, [pr_hm_norm,  pr_hm_scores,gt_hm_norm, gt_hm_scores], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'fcn_norm_loss']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn_model.keras_model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "source": [
    "### Training FPN, RPN and MRCNN heads using  Keras.model.fit_generator()\n",
    "\n",
    "print(config.BATCH_SIZE)\n",
    "print(model.config.BATCH_SIZE)\n",
    "print(model.config.LEARNING_RATE)\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T21:37:28.251199Z",
     "start_time": "2018-06-05T21:11:11.215070Z"
    },
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=config.LEARNING_RATE, \n",
    "# #             epochs = 69,\n",
    "#             epochs_to_run =2, \n",
    "#             layers='heads')\n",
    "## Last run prior to FCN training was 3699, last checkpoint was 3892  ...start at 3899\n",
    "\n",
    "train_layers = [ 'mrcnn', 'fpn','rpn']\n",
    "loss_names   = [ \"rpn_class_loss\", \"rpn_bbox_loss\" , \"mrcnn_class_loss\", \"mrcnn_bbox_loss\"]\n",
    "model.epoch = 1233\n",
    "model.config.LEARNING_RATE = 1.0e-4\n",
    "model.config.STEPS_PER_EPOCH = 7\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=model.config.LEARNING_RATE, \n",
    "            epochs_to_run =3000, \n",
    "#             epochs = 25,            \n",
    "#             batch_size = 0\n",
    "#             steps_per_epoch = 0 \n",
    "            layers = train_layers,\n",
    "            losses = loss_names,\n",
    "            min_LR = 1.0e-6,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train FCN head layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T12:39:18.755289Z",
     "start_time": "2018-06-07T12:34:14.060639Z"
    },
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=config.LEARNING_RATE, \n",
    "# #             epochs = 69,\n",
    "#             epochs_to_run =2, \n",
    "#             layers='heads')\n",
    "\n",
    "## Last run prior to FCN training was 3699, last checkpoint was 3892\n",
    "\n",
    "train_layers = ['fcn']\n",
    "loss_names   = [  \"fcn_norm_loss\"]\n",
    "model.epoch = 1668\n",
    "model.config.LEARNING_RATE = 1.0e-4\n",
    "model.config.STEPS_PER_EPOCH = 8 \n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=model.config.LEARNING_RATE, \n",
    "            epochs_to_run = 500, \n",
    "#             epochs = 25,            \n",
    "#             batch_size = 6,\n",
    "#             steps_per_epoch = 0 \n",
    "            layers = train_layers,\n",
    "            losses = loss_names,\n",
    "            min_LR = 1.0e-9\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T18:17:32.353508Z",
     "start_time": "2018-05-20T18:17:32.121048Z"
    }
   },
   "outputs": [],
   "source": [
    "model.keras_model.losses\n",
    "print(model.keras_model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## - Training heads using train_in_batches ()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-28T15:03:53.709099Z",
     "start_time": "2018-04-28T15:02:36.185321Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE/6, \n",
    "            epochs_to_run = 3,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Fine Tuning\n",
    "Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=211,\n",
    "            layers=\"all\")\n",
    "\n",
    "# train_layers = ['fcn']\n",
    "# loss_names   = [  \"fcn_norm_loss\"]\n",
    "# model.epoch = 208\n",
    "# model.config.LEARNING_RATE = 1.0e-4\n",
    "# model.config.STEPS_PER_EPOCH = 8 \n",
    "\n",
    "# model.train(dataset_train, dataset_val, \n",
    "#             learning_rate=model.config.LEARNING_RATE, \n",
    "#             epochs_to_run = 500, \n",
    "# #             epochs = 25,            \n",
    "# #             batch_size = 6,\n",
    "# #             steps_per_epoch = 0 \n",
    "#             layers = train_layers,\n",
    "#             losses = loss_names,\n",
    "#             min_LR = 1.0e-7\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes_post_training.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T18:25:16.962148Z",
     "start_time": "2018-05-20T18:25:16.737938Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model.keras_model.summary(line_length=132, positions=[0.30,0.75, .83, 1. ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:10:55.871863Z",
     "start_time": "2018-06-05T14:10:51.289152Z"
    },
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)\n",
    "imgmeta_idx = model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "for img_idx in range(config.BATCH_SIZE):\n",
    "    image_id = img_meta[img_idx,0]\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print('Image id: ',image_id)\n",
    "    print('Image meta', img_meta[img_idx])\n",
    "    print('Classes (1: circle, 2: square, 3: triangle ): ',class_ids)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:10:58.357215Z",
     "start_time": "2018-06-05T14:10:58.128846Z"
    },
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "model.layer_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T14:11:29.002604Z",
     "start_time": "2018-06-05T14:11:21.360692Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_batch_x, train_batch_y = next(train_generator)\n",
    "\n",
    "model_output = get_layer_output_2(model.keras_model, train_batch_x, 1)\n",
    "# model_output = get_layer_output_1(model.keras_model, train_batch_x, [ 26], 1)\n",
    "\n",
    "print(len(model_output))\n",
    "\n",
    "# rpn_class_loss            = model_output[0]          # layer: 11   shape: (1, 1)\n",
    "# rpn_bbox_loss             = model_output[1]          # layer: 12   shape: (1, 1)\n",
    "# mrcnn_class_loss          = model_output[2]          # layer: 13   shape: (1, 1)\n",
    "# mrcnn_bbox_loss           = model_output[3]          # layer: 14   shape: (1, 1)\n",
    "# fcn_normalized_loss       = model_output[0]          # layer: 26   shape: (1, 1)\n",
    "\n",
    "# print(type(output_rois))\n",
    "for i in model_output:\n",
    "    print( i.shape)\n",
    "# print('FCN Normalized Loss is :', fcn_normalized_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T18:40:52.258442Z",
     "start_time": "2018-05-20T18:40:52.031879Z"
    },
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_image      =  train_batch_x[0]\n",
    "input_image_meta =  train_batch_x[1]\n",
    "input_rpn_match  =  train_batch_x[2]\n",
    "input_rpn_bbox   =  train_batch_x[3]\n",
    "input_gt_class_ids = train_batch_x[4]\n",
    "input_gt_bboxes    = train_batch_x[5]\n",
    "input_gt_masks     = train_batch_x[6]\n",
    "print(' Input image shape is :', input_image.shape)\n",
    "h, w = input_image.shape[1], input_image.shape[2]      #  tf.shape(input_image)[1], tf.shape(input_image)[2]\n",
    "input_normlzd_gt_bboxes = tf.identity(input_gt_bboxes / [h,w,h,w])\n",
    "\n",
    "# gt_masks   =  train_batch_x[6]\n",
    "print(' input_rpn_match    ', input_rpn_match.shape)\n",
    "print(' input_rpn_bbox     ', input_rpn_bbox.shape)\n",
    "print(' input_gt_class_ids ', input_gt_class_ids.shape)\n",
    "print(' input_gt_bboxes    ', input_gt_bboxes.shape)\n",
    "print(' input_normlzd_gt_bboxes    ', input_normlzd_gt_bboxes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
