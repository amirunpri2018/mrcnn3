{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "### Notes from implementation\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. \n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from jupyterthemes import jtplot\n",
    "\n",
    "# choose which theme to inherit plotting style from\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "# jtplot.style(theme='onedork')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\TF_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.4.0   Keras Version : 2.1.3 \n",
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                7\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pprint\n",
    "import keras.backend as KB\n",
    "sys.path.append('../')\n",
    "\n",
    "import mrcnn.model     as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "import mrcnn.shapes    as shapes\n",
    "from mrcnn.config      import Config\n",
    "from mrcnn.model       import log\n",
    "from mrcnn.dataset     import Dataset \n",
    "# from mrcnn.pc_layer    import PCTensor\n",
    "from mrcnn.pc_layer   import PCNLayer\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "MODEL_PATH = 'E:\\Models'\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(MODEL_PATH, \"mrcnn_logs\")\n",
    "# Path to COCO trained weights\n",
    "COCO_MODEL_PATH   = os.path.join(MODEL_PATH, \"mask_rcnn_coco.h5\")\n",
    "RESNET_MODEL_PATH = os.path.join(MODEL_PATH, \"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "\n",
    "print(\"Tensorflow Version: {}   Keras Version : {} \".format(tf.__version__,keras.__version__))\n",
    "pp = pprint.PrettyPrinter(indent=2, width=100)\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "# Build configuration object -----------------------------------------------\n",
    "config = shapes.ShapesConfig()\n",
    "config.BATCH_SIZE      = 2                    #Batch size is 2 (# GPUs * images/GPU).\n",
    "config.IMAGES_PER_GPU  = 2\n",
    "config.STEPS_PER_EPOCH = 7\n",
    "# config.IMAGES_PER_GPU  = 1\n",
    "config.display() \n",
    "\n",
    "# Build shape dataset        -----------------------------------------------\n",
    "\n",
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "\n",
    "# Training dataset\n",
    "# generate 500 shapes \n",
    "dataset_train = shapes.ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = shapes.ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Load and display random samples\n",
    "# image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "# for image_id in [3]:\n",
    "#     image = dataset_train.load_image(image_id)\n",
    "#     mask, class_ids = dataset_train.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180401T1447\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      " IMAGE SHAPE is : 128    128\n",
      "<class 'list'>\n",
      "Tensor(\"rpn_class_logits/concat:0\", shape=(?, ?, 2), dtype=float32) rpn_class_logits/concat:0\n",
      "Tensor(\"rpn_class/concat:0\", shape=(?, ?, 2), dtype=float32) rpn_class/concat:0\n",
      "Tensor(\"rpn_bbox/concat:0\", shape=(?, ?, 4), dtype=float32) rpn_bbox/concat:0\n",
      "Proposal Layer init complete. Size of anchors:  (4092, 4)\n",
      ">>> Detection Target Layer : initialization\n",
      ">>> Detection Target Layer : call \n",
      ">>> detection_targets_graph \n",
      "     overlaps_graph: shape of boxes1 before reshape:  (?, ?)\n",
      "     overlaps_graph: shape of boxes2 before reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes1 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes2 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of b1_y1 after reshape:  (?, 1)\n",
      "Shape of overlaps Tensor(\"proposal_targets/Shape_5:0\", shape=(2,), dtype=int32)\n",
      ">>> detection_targets_graph \n",
      "     overlaps_graph: shape of boxes1 before reshape:  (?, ?)\n",
      "     overlaps_graph: shape of boxes2 before reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes1 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes2 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of b1_y1 after reshape:  (?, 1)\n",
      "Shape of overlaps Tensor(\"proposal_targets/Shape_10:0\", shape=(2,), dtype=int32)\n",
      ">>> detection_targets_graph \n",
      "     overlaps_graph: shape of boxes1 before reshape:  (?, ?)\n",
      "     overlaps_graph: shape of boxes2 before reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes1 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes2 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of b1_y1 after reshape:  (?, 1)\n",
      "Shape of overlaps Tensor(\"proposal_targets/Shape_20:0\", shape=(2,), dtype=int32)\n",
      ">>> detection_targets_graph \n",
      "     overlaps_graph: shape of boxes1 before reshape:  (?, ?)\n",
      "     overlaps_graph: shape of boxes2 before reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes1 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of boxes2 after reshape:  (?, 4)\n",
      "     overlaps_graph: shape of b1_y1 after reshape:  (?, 1)\n",
      "Shape of overlaps Tensor(\"proposal_targets/Shape_25:0\", shape=(2,), dtype=int32)\n",
      ">>> Detection Target Layer : return call  <class 'list'>\n",
      ">>> PCN Layer : initialization\n",
      ">>> PCN Layer : call\n",
      "     mrcnn_class.shape    : (?, 32, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     mrcnn_bbox.shape     : (?, 32, 4, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     output_rois.shape    : (?, 32, 28, 28, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      ">>> PCN Layer : call end  \n",
      ">>> MaskRCNN build complete\n",
      ">>> MaskRCNN initialization complete\n",
      "E:\\Models\\mask_rcnn_coco.h5\n",
      " Checkpoint folder:  E:\\Models\\mrcnn_logs\n",
      "find_last info:   dir_name: E:\\Models\\mrcnn_logs\\shapes20180313T1856\n",
      "find_last info: checkpoint: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "('E:\\\\Models\\\\mrcnn_logs\\\\shapes20180313T1856', 'E:\\\\Models\\\\mrcnn_logs\\\\shapes20180313T1856\\\\mask_rcnn_shapes_0231.h5')\n",
      "find_last info:   dir_name: E:\\Models\\mrcnn_logs\\shapes20180313T1856\n",
      "find_last info: checkpoint: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "load_weights: Loading weights from: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "load_weights: Log directory set to : E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0231.h5\n",
      "set_log_dir:  model_path (input) is : E:/Models/mrcnn_logs/shapes20180313T1856/mask_rcnn_shapes_0231.h5  \n",
      "self.epoch set to 232  (Next epoch to run)\n",
      "set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      ">>> Load weights complete\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(model)\n",
    "# Create model in training mode\n",
    "# MODEL_DIR = os.path.join(MODEL_PATH, \"mrcnn_logs\")\n",
    "import  gc\n",
    "# del history\n",
    "try :\n",
    "    del model\n",
    "except: \n",
    "    pass\n",
    " \n",
    "gc.collect()\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "\n",
    "#model.keras_model.summary(line_length = 120)\n",
    "\n",
    "# print(MODEL_PATH)\n",
    "print(COCO_MODEL_PATH)\n",
    "# print(RESNET_MODEL_PATH)\n",
    "print(' Checkpoint folder: ', MODEL_DIR)\n",
    "print(model.find_last())\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"last\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "#     loc=model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "    loc=model.load_weights(RESNET_MODEL_PATH, by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    \n",
    "    # See README for instructions to download the COCO weights\n",
    "    loc=model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    loc= model.load_weights(model.find_last()[1], by_name=True)\n",
    "print('>>> Load weights complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=model.config.BATCH_SIZE,\n",
    "                                 augment = False)\n",
    "val_generator = data_generator(dataset_val, model.config, shuffle=True, \n",
    "                                batch_size=model.config.BATCH_SIZE,\n",
    "                                augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process outside of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile with learing rate; 0.001 Learning Moementum: 0.9 \n",
      "Checkpoint Folder:  E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_{epoch:04d}.h5 \n",
      "\n",
      "Selecting layers to train\n",
      "Layer    Layer Name               Layer Type\n",
      "174  fpn_c5p5               (Conv2D)\n",
      "176  fpn_c4p4               (Conv2D)\n",
      "179  fpn_c3p3               (Conv2D)\n",
      "182  fpn_c2p2               (Conv2D)\n",
      "184  fpn_p5                 (Conv2D)\n",
      "185  fpn_p2                 (Conv2D)\n",
      "186  fpn_p3                 (Conv2D)\n",
      "187  fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "      1  rpn_conv_shared        (Conv2D)\n",
      "      2  rpn_class_raw          (Conv2D)\n",
      "      4  rpn_bbox_pred          (Conv2D)\n",
      "199  mrcnn_mask_conv1       (TimeDistributed)\n",
      "200  mrcnn_mask_bn1         (TimeDistributed)\n",
      "202  mrcnn_mask_conv2       (TimeDistributed)\n",
      "204  mrcnn_mask_bn2         (TimeDistributed)\n",
      "205  mrcnn_class_conv1      (TimeDistributed)\n",
      "207  mrcnn_class_bn1        (TimeDistributed)\n",
      "208  mrcnn_mask_conv3       (TimeDistributed)\n",
      "210  mrcnn_mask_bn3         (TimeDistributed)\n",
      "211  mrcnn_class_conv2      (TimeDistributed)\n",
      "213  mrcnn_class_bn2        (TimeDistributed)\n",
      "214  mrcnn_mask_conv4       (TimeDistributed)\n",
      "216  mrcnn_mask_bn4         (TimeDistributed)\n",
      "219  mrcnn_class_logits     (TimeDistributed)\n",
      "220  mrcnn_bbox_fc          (TimeDistributed)\n",
      "221  mrcnn_mask_deconv      (TimeDistributed)\n",
      "226  mrcnn_mask             (TimeDistributed)\n",
      "Callback_metrics are:  ( val+_get_deduped_metrics_names() )\n",
      " ['loss', 'rpn_class_loss', 'rpn_bbox_loss', 'mrcnn_class_loss', 'mrcnn_bbox_loss', 'mrcnn_mask_loss', 'val_loss', 'val_rpn_class_loss', 'val_rpn_bbox_loss', 'val_mrcnn_class_loss', 'val_mrcnn_bbox_loss', 'val_mrcnn_mask_loss']\n",
      " Learning phase values is L  1\n",
      "\n",
      " Metrics (_get_deduped_metrics_names():) \n",
      "[ 'loss',\n",
      "  'rpn_class_loss',\n",
      "  'rpn_bbox_loss',\n",
      "  'mrcnn_class_loss',\n",
      "  'mrcnn_bbox_loss',\n",
      "  'mrcnn_mask_loss']\n",
      "\n",
      " Outputs: \n",
      "[ <tf.Tensor 'output_rois/mul:0' shape=(2, ?, ?) dtype=float32>,\n",
      "  <tf.Tensor 'cntxt_layer/PyFunc:0' shape=<unknown> dtype=float32>,\n",
      "  <tf.Tensor 'cntxt_layer/PyFunc:1' shape=<unknown> dtype=float32>,\n",
      "  <tf.Tensor 'cntxt_layer/PyFunc:2' shape=<unknown> dtype=float32>,\n",
      "  <tf.Tensor 'cntxt_layer/PyFunc:3' shape=<unknown> dtype=int16>,\n",
      "  <tf.Tensor 'cntxt_layer/PyFunc:4' shape=<unknown> dtype=float32>,\n",
      "  <tf.Tensor 'cntxt_layer/PyFunc:5' shape=<unknown> dtype=int16>,\n",
      "  <tf.Tensor 'rpn_class_logits/concat:0' shape=(?, ?, 2) dtype=float32>,\n",
      "  <tf.Tensor 'proposal_rois/packed_2:0' shape=(2, ?, ?) dtype=float32>,\n",
      "  <tf.Tensor 'rpn_class/concat:0' shape=(?, ?, 2) dtype=float32>,\n",
      "  <tf.Tensor 'rpn_bbox/concat:0' shape=(?, ?, 4) dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_class_logits/Reshape_1:0' shape=(?, 32, 4) dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_class/Reshape_1:0' shape=(?, 32, 4) dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_bbox/Reshape:0' shape=(?, 32, 4, 4) dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_mask/Reshape_1:0' shape=(?, 32, 28, 28, 4) dtype=float32>,\n",
      "  <tf.Tensor 'rpn_class_loss/cond/Merge:0' shape=() dtype=float32>,\n",
      "  <tf.Tensor 'rpn_bbox_loss/cond/Merge:0' shape=() dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_class_loss/truediv:0' shape=() dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_bbox_loss/Reshape_3:0' shape=(1, 1) dtype=float32>,\n",
      "  <tf.Tensor 'mrcnn_mask_loss/Reshape_3:0' shape=(1, 1) dtype=float32>]\n",
      "\n",
      " Losses (model.metrics_names): \n",
      "[ 'loss',\n",
      "  'rpn_class_loss',\n",
      "  'rpn_bbox_loss',\n",
      "  'mrcnn_class_loss',\n",
      "  'mrcnn_bbox_loss',\n",
      "  'mrcnn_mask_loss']\n"
     ]
    }
   ],
   "source": [
    "model.compile_only(learning_rate=config.LEARNING_RATE, layers='heads')\n",
    "# print(KB.eval(KB.learning_phase()))\n",
    "KB.set_learning_phase(1)\n",
    "print(' Learning phase values is L ' ,KB.learning_phase())\n",
    "mm = model.keras_model\n",
    "print('\\n Metrics (_get_deduped_metrics_names():) ') \n",
    "pp.pprint(mm._get_deduped_metrics_names())\n",
    "print('\\n Outputs: ') \n",
    "pp.pprint(mm.outputs)\n",
    "# pp.pprint(mm.fit_generator.__dict__['__wrapped__'])\n",
    "\n",
    "print('\\n Losses (model.metrics_names): ') \n",
    "# pp.pprint(mm.losses)\n",
    "pp.pprint(mm.metrics_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get next shapes from generator and display loaded shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Mask information (shape, (color rgb), (x_ctr, y_ctr, size) ): \n",
      "[('circle', (116, 190, 144), (23, 43, 23)), ('triangle', (5, 98, 31), (83, 32, 32))]\n",
      " Shapes obj mask shape is : (128, 128, 2)\n",
      "Load Mask information (shape, (color rgb), (x_ctr, y_ctr, size) ): \n",
      "[ ('triangle', (34, 59, 29), (41, 52, 22)),\n",
      "  ('triangle', (202, 112, 49), (63, 102, 20)),\n",
      "  ('square', (236, 140, 3), (60, 80, 31))]\n",
      " Shapes obj mask shape is : (128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id:  438\n",
      "Load Mask information (shape, (color rgb), (x_ctr, y_ctr, size) ): \n",
      "[('circle', (116, 190, 144), (23, 43, 23)), ('triangle', (5, 98, 31), (83, 32, 32))]\n",
      " Shapes obj mask shape is : (128, 128, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2933fee7ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id:  262\n",
      "Load Mask information (shape, (color rgb), (x_ctr, y_ctr, size) ): \n",
      "[ ('triangle', (34, 59, 29), (41, 52, 22)),\n",
      "  ('triangle', (202, 112, 49), (63, 102, 20)),\n",
      "  ('square', (236, 140, 3), (60, 80, 31))]\n",
      " Shapes obj mask shape is : (128, 128, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2933ff97908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgmeta_idx = mm.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot mask in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=99999, linewidth=2000)\n",
    "# print(np.array2string(mask[...,0],max_line_width=2000,separator=''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outmask0 = layers_out[14][0,:,:,:,1] ##  mrcnn_mask\n",
    "np.max(outmask0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=211,\n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_bbox)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training head using  Keras.model.fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=69, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training heads using train_on_batch()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run = 2,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate one training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "np.set_printoptions(linewidth=100)\n",
    "learning_rate=model.config.LEARNING_RATE\n",
    "epochs_to_run = 2\n",
    "layers='heads'\n",
    "batch_size = 0\n",
    "steps_per_epoch = 0\n",
    "# assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "# Pre-defined layer regular expressions\n",
    "layer_regex = {\n",
    "    # all layers but the backbone\n",
    "    \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # From a specific Resnet stage and up\n",
    "    \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # All layers\n",
    "    \"all\": \".*\",\n",
    "}\n",
    "\n",
    "if layers in layer_regex.keys():\n",
    "    layers = layer_regex[layers]\n",
    "if batch_size == 0 :\n",
    "    batch_size = model.config.BATCH_SIZE            \n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = model.config.STEPS_PER_EPOCH\n",
    "\n",
    "# Data generators\n",
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size)\n",
    "val_generator   = data_generator(dataset_val, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 augment=False)\n",
    "\n",
    "# Train\n",
    "log(\"Last epoch completed : {} \".format(model.epoch))\n",
    "log(\"Starting from epoch {} for {} epochs. LR={}\".format(model.epoch, epochs_to_run, learning_rate))\n",
    "log(\"Steps per epoch:    {} \".format(steps_per_epoch))\n",
    "log(\"Batchsize      :    {} \".format(batch_size))\n",
    "log(\"Checkpoint Folder:  {} \".format(model.checkpoint_path))\n",
    "epochs = model.epoch + epochs_to_run\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "if not gfile.IsDirectory(model.log_dir):\n",
    "    log('Creating checkpoint folder')\n",
    "    gfile.MakeDirs(model.log_dir)\n",
    "else:\n",
    "    log('Checkpoint folder already exists')\n",
    "\n",
    "model.set_trainable(layers)            \n",
    "model.compile(learning_rate, model.config.LEARNING_MOMENTUM)        \n",
    "\n",
    "out_labels = model.keras_model._get_deduped_metrics_names()\n",
    "callback_metrics = out_labels + ['val_' + n for n in out_labels]\n",
    "\n",
    "progbar = keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "progbar.set_model(model.keras_model)\n",
    "progbar.set_params({\n",
    "    'epochs': epochs,\n",
    "    'steps': steps_per_epoch,\n",
    "    'verbose': 1,\n",
    "    'do_validation': False,\n",
    "    'metrics': callback_metrics,\n",
    "})\n",
    "\n",
    "progbar.set_model(model.keras_model) \n",
    "\n",
    "chkpoint = keras.callbacks.ModelCheckpoint(model.checkpoint_path, \n",
    "                                           monitor='loss', verbose=1, save_best_only = True, save_weights_only=True)\n",
    "chkpoint.set_model(model.keras_model)\n",
    "\n",
    "progbar.on_train_begin()\n",
    "epoch_idx = model.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch_idx >= epochs:\n",
    "    print('Final epoch {} has already completed - Training will not proceed'.format(epochs))\n",
    "\n",
    "# while epoch_idx < epochs :\n",
    "progbar.on_epoch_begin(epoch_idx)\n",
    "steps_index = 0\n",
    "# for steps_index in range(steps_per_epoch):\n",
    "\n",
    "batch_logs = {}\n",
    "print(' self.epoch {}   epochs {}  step {} '.format(model.epoch, epochs, steps_index))\n",
    "batch_logs['batch'] = steps_index\n",
    "batch_logs['size']  = batch_size\n",
    "progbar.on_batch_begin(steps_index, batch_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx= model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta  =  train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "outs = model.keras_model.train_on_batch(train_batch_x, train_batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate one training iteration - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(outs, list):\n",
    "    outs = [outs]\n",
    "for l, o in zip(out_labels, outs):\n",
    "    batch_logs[l] = o\n",
    "\n",
    "    progbar.on_batch_end(steps_index, batch_logs)\n",
    "\n",
    "        # print(outs)\n",
    "    progbar.on_epoch_end(epoch_idx, {})\n",
    "    # if (epoch_idx % 10) == 0:\n",
    "    chkpoint.on_epoch_end(epoch_idx  , batch_logs)\n",
    "    epoch_idx += 1\n",
    "\n",
    "# if epoch_idx != self.epoch:\n",
    "# chkpoint.on_epoch_end(epoch_idx -1, batch_logs)\n",
    "model.epoch = max(epoch_idx - 1, epochs)\n",
    "\n",
    "print('Final : self.epoch {}   epochs {}'.format(model.epoch, epochs))\n",
    "# end if (else)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [TF_gpu]",
   "language": "python",
   "name": "Python [TF_gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
