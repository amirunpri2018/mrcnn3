{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "### Notes from implementation\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:26:11.062541Z",
     "start_time": "2018-04-15T18:26:10.831930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import  gc\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pprint\n",
    "import keras.backend as KB\n",
    "sys.path.append('../')\n",
    "\n",
    "import mrcnn.model     as modellib\n",
    "import mrcnn.visualize as visualize\n",
    "import mrcnn.shapes    as shapes\n",
    "from mrcnn.config      import Config\n",
    "from mrcnn.model       import log\n",
    "from mrcnn.dataset     import Dataset \n",
    "# from mrcnn.pc_layer    import PCTensor\n",
    "# from mrcnn.pc_layer   import PCNLayer\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "MODEL_PATH = 'E:\\Models'\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(MODEL_PATH, \"mrcnn_logs\")\n",
    "# Path to COCO trained weights\n",
    "COCO_MODEL_PATH   = os.path.join(MODEL_PATH, \"mask_rcnn_coco.h5\")\n",
    "RESNET_MODEL_PATH = os.path.join(MODEL_PATH, \"resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "\n",
    "print(\"Tensorflow Version: {}   Keras Version : {} \".format(tf.__version__,keras.__version__))\n",
    "pp = pprint.PrettyPrinter(indent=2, width=100)\n",
    "np.set_printoptions(linewidth=100)\n",
    "\n",
    "# Build configuration object -----------------------------------------------\n",
    "config = shapes.ShapesConfig()\n",
    "config.BATCH_SIZE      = 4                   #Batch size is 2 (# GPUs * images/GPU).\n",
    "config.IMAGES_PER_GPU  = 4\n",
    "config.STEPS_PER_EPOCH = 2\n",
    "# config.IMAGES_PER_GPU  = 1\n",
    "config.FCN_INPUT_SHAPE = config.IMAGE_SHAPE[0:2]\n",
    "config.display() \n",
    "\n",
    "# Build shape dataset        -----------------------------------------------\n",
    "\n",
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "\n",
    "# Training dataset\n",
    "# generate 500 shapes \n",
    "dataset_train = shapes.ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = shapes.ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Load and display random samples\n",
    "# image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "# for image_id in [3]:\n",
    "#     image = dataset_train.load_image(image_id)\n",
    "#     mask, class_ids = dataset_train.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "print(' COCO Model Path       : ', COCO_MODEL_PATH)\n",
    "print(' Checkpoint folder Path: ', MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:26:14.098642Z",
     "start_time": "2018-04-15T18:26:13.874023Z"
    }
   },
   "outputs": [],
   "source": [
    "# del dataset_train,dataset_val\n",
    "# del model\n",
    "# gc.collect()\n",
    "# sess = KB.get_session()\n",
    "# sess.close()\n",
    "# import tensorflow as tf\n",
    "# # print(\"Tensorflow Version: {}   Keras Version : {} \".format(tf.__version__))\n",
    "# dir(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process outside of training \n",
    "\n",
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:27:57.808988Z",
     "start_time": "2018-04-15T18:26:19.179137Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Set_log_dir() -- model dir is  E:\\Models\\mrcnn_logs\n",
      "    set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180415T2026\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      ">>> Generate pyramid anchors \n",
      "    Size of anchor array is : (4092, 4)\n",
      ">>> RPN Outputs  <class 'list'>\n",
      "      rpn_class_logits_1/concat:0\n",
      "      rpn_class_1/concat:0\n",
      "      rpn_bbox_1/concat:0\n",
      ">>> Proposal Layer init complete. Size of anchors:  (4092, 4)\n",
      "     Scores :  (4, 4092)\n",
      "     Deltas :  (4, 4092, 4)\n",
      "     Anchors:  (4, 4092, 4)\n",
      "     Boxes shape / type after processing:  (4, 4092, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      ">>> Detection Target Layer : initialization\n",
      ">>> Detection Target Layer : call  <class 'list'> 4\n",
      "     proposals.shape    : (4, ?, ?) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     gt_class_ids.shape : (?, ?) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     gt_bboxes.shape    : (?, ?, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     gt_masks.shape     : (?, 56, 56, ?) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      ">>> Detection Target Layer : return call  <class 'list'> 4\n",
      "     output 0  shape (4, ?, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "     output 1  shape (4, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "     output 2  shape (4, ?, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      "     output 3  shape (4, ?, ?, ?)  type <class 'tensorflow.python.framework.ops.Tensor'> \n",
      ">>> PCN Layer : initialization\n",
      ">>> PCN Layer : call  <class 'list'> 5\n",
      "     mrcnn_class.shape    : (?, 32, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     mrcnn_bbox.shape     : (?, 32, 4, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     output_rois.shape    : (?, ?) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      ">>> PCN Layer TF : initialization\n",
      ">>> PCN Layer TF: call  <class 'list'> 5\n",
      "     mrcnn_class.shape    : (?, 32, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     mrcnn_bbox.shape     : (?, 32, 4, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     output_rois.shape    : (4, ?, ?) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     gt_class_ids.shape   : (?, ?) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "     gt_bboxes.shape      : (?, ?, 4) <class 'tensorflow.python.framework.ops.Tensor'>\n",
      ">>> build_predictions_tf\n",
      " -- pred_tensor tf ------------------------------\n",
      "    resulting tensor : a_boxes_3d <class 'tensorflow.python.framework.ops.Tensor'> (4, 32, 7)\n",
      "    pred_scatter shape is  (4, 4, 32, 7) Tensor(\"cntxt_layer_2_1/ScatterNd:0\", shape=(4, 4, 32, 7), dtype=float32)\n",
      "    sort inds shape :  (4, 4, 32)\n",
      "    class_grid   <class 'tensorflow.python.framework.ops.Tensor'> shape (4, 4, 32)\n",
      "    batch_grid   <class 'tensorflow.python.framework.ops.Tensor'> shape (4, 4, 32)\n",
      "    roi_grid shape (4, 4, 32) roi_grid_exp shape  (4, 4, 32, 1)\n",
      "    gather_inds  <class 'tensorflow.python.framework.ops.Tensor'> shape (4, 4, 32, 3)\n",
      "    pred_tensor (gathered)  :  (4, 4, 32, 7)\n",
      "    -- pred_tensor results (A-boxes sorted by score ----\n",
      "    final pred_tensor shape  :  (4, 4, 32, 8)\n",
      "    final pred_cls_cnt shape :  (4, 4)\n",
      "    complete\n",
      ">>> build_ground_truth_tf\n",
      "    gt_class_ids shape :  (?, ?)     notm_gt_bbox.shape  :  (?, ?, 4)\n",
      "    gt_classes_exp shape  (?, ?, 1)\n",
      "    pred_ scores shape  (?, ?)\n",
      "    bbox_grid  shape   (4, 100)\n",
      "    batch_grid  shape  (4, 100)\n",
      "    bbox_idx shape    (4, 100, 1)\n",
      "    gt_array shape    (4, 100, 7)\n",
      "    class shape       (?, ?)\n",
      "    bbox_grid   shape  (4, 100)\n",
      "    batch_grid shape  (4, 100)\n",
      "    scatter_ind shape  (4, 100, 3)\n",
      "    gt_scatter shape  (4, 4, 100, 7)\n",
      "    sort inds shape :  (4, 4, 100)\n",
      "    class_grid  shape  (4, 4, 100)\n",
      "    batch_grid  shape  (4, 4, 100)\n",
      "    bbox_grid   shape  (4, 4, 100)  bbox_grid_exp shape  (4, 4, 100, 1)\n",
      "    gather_inds shape  (4, 4, 100, 3)\n",
      "    gt_tensor (gathered)   :  (4, 4, 100, 8)\n",
      "    final gt_tensor shape  :  (4, 4, 100, 8)\n",
      "    final gt_cls_cnt shape :  (4, 4)\n",
      "    complete\n",
      " Build Gaussian np for detected rois =========================\n",
      ">>> build_gaussian_tf \n",
      "    in_tensor shape :  (4, 4, 32, 5)\n",
      "    num of bboxes per class is :  Tensor(\"cntxt_layer_2_1/ToInt32_2/x:0\", shape=(), dtype=int32)\n",
      "    pred_tensor shape is       :  (4, 4, 32, 5)\n",
      "   pt2_sum shape  (4, 4, 32)\n",
      "   pt2_ind shape  (?, 3)\n",
      "   dense shape  (?, 5)\n",
      "   dense shape  (?, 6)\n",
      "   -- Build Stacked output from dynamically partitioned lists --------------\n",
      "\n",
      "   ===> list item # 0\n",
      "   stacked_list[img] shape:  Tensor(\"cntxt_layer_2_1/strided_slice_9:0\", shape=(), dtype=int32)\n",
      "   tensor_list item pos padding : Tensor(\"cntxt_layer_2_1/Shape_1:0\", shape=(2,), dtype=int32)\n",
      "\n",
      "   ===> list item # 1\n",
      "   stacked_list[img] shape:  Tensor(\"cntxt_layer_2_1/strided_slice_10:0\", shape=(), dtype=int32)\n",
      "   tensor_list item pos padding : Tensor(\"cntxt_layer_2_1/Shape_3:0\", shape=(2,), dtype=int32)\n",
      "\n",
      "   ===> list item # 2\n",
      "   stacked_list[img] shape:  Tensor(\"cntxt_layer_2_1/strided_slice_11:0\", shape=(), dtype=int32)\n",
      "   tensor_list item pos padding : Tensor(\"cntxt_layer_2_1/Shape_5:0\", shape=(2,), dtype=int32)\n",
      "\n",
      "   ===> list item # 3\n",
      "   stacked_list[img] shape:  Tensor(\"cntxt_layer_2_1/strided_slice_12:0\", shape=(), dtype=int32)\n",
      "   tensor_list item pos padding : Tensor(\"cntxt_layer_2_1/Shape_7:0\", shape=(2,), dtype=int32)\n",
      "\n",
      "stacked output tensor shape :  Tensor(\"cntxt_layer_2_1/Shape_8:0\", shape=(3,), dtype=int32) (4, ?, ?) (4, ?, ?)\n",
      "   stacked tensor :  Tensor(\"cntxt_layer_2_1/Shape_9:0\", shape=(3,), dtype=int32) (4, ?, ?) (4, ?, ?)\n",
      "(128, 128) (128, 128)\n",
      "   ones:  (4, 32, 1, 1)\n",
      "   after transpose  (128, 128, 4, 32, 2)\n",
      "ps.shape is  (4, ?, ?)\n",
      "   means shape  (4, ?, 2)    (4, ?, 2)\n",
      "   covar shape  (4, ?, 2)    (4, ?, 2)\n",
      "   from MVN :  \t mns shape      : (4, ?, 2) (4, ?, 2)  \t cov shape :  (4, ?, 2) (4, ?, 2)\n",
      "   from MVN :  \t mean shape     : (4, ?, 2) \t stddev shape (4, ?, 2)\n",
      "   from MVN :  \t mean shape     : (4, ?, 2) \t stddev shape (4, ?, 2)\n",
      "   from MVN :  \t mvn.batch_shape: (?, ?) \t mvn.event_shape  (2,)\n",
      "   Linear OP shape       (4, ?, 2, 2)  Linear Op batch shape  (4, ?)\n",
      "   Linear op Range Dim   2\n",
      "   Linear op Domain Dim  2\n",
      "   >> input to MVN.PROB: pos_grid (meshgrid) shape:  (128, 128, 4, 32, 2)\n",
      "   << output probabilities shape: (128, 128, 4, 32)\n",
      "   trans_grid shape   :  (4, 32, 128, 128)\n",
      "   gaussian_grid      :  (4, 32, 128, 128)\n",
      "   class shape        :  (4, ?)\n",
      "   roi_grid shape     :  (4, 32)\n",
      "   batch_grid shape   :  (4, 32)\n",
      "   scatter_classes    :  (4, 32, 3)\n",
      "   gaussian scattered :  (4, 4, 32, 128, 128)\n",
      "   gaussian_sum shape :  (4, 4, 128, 128)\n",
      "    means  :  (4, ?, 2)     covar :  (4, ?, 2)\n",
      " shape of pcn_tnesor2, cls_cnt2    (4, 4, 32, 8) (4, 4)\n",
      " shape of gt_tnesor2, gt_cls_cnt2  (4, 4, 100, 8) (4, 4)\n",
      " shape of pcn_scatter is    <unknown>\n",
      " shape of pcn_scatter_2 is  (4, 4, 32, 128, 128)\n",
      " shape of pcn_gaussian is   <unknown>\n",
      " shape of pcn_gaussian_2    (4, 4, 128, 128)\n",
      ">>> rpn_bbox_loss_graph\n",
      "    rpn_match size : (?, ?)\n",
      "    rpn_bbox  size : (?, ?, 4)\n",
      "    tf default session:  <tensorflow.python.client.session.InteractiveSession object at 0x000001D899973EB8>\n",
      ">>> rpn_bbox_loss_graph\n",
      "    rpn_match size : (?, ?)\n",
      "    rpn_bbox  size : (?, ?, 4)\n",
      "    tf default session:  <tensorflow.python.client.session.InteractiveSession object at 0x000001D899973EB8>\n",
      ">>> rpn_bbox_loss_graph_old\n",
      "    rpn_match size    :  (?, ?)\n",
      "    rpn_bbox  size    :  (?, ?, 4)\n",
      "    target_bbox size n:  (?, ?, 4)\n",
      ">>> rpn_bbox_loss_graph_old\n",
      "    rpn_match size    :  (?, ?)\n",
      "    rpn_bbox  size    :  (?, ?, 4)\n",
      "    target_bbox size n:  (?, ?, 4)\n",
      ">>> mrcnn_class_loss_graph \n",
      "    target_class_ids  size : (4, ?)\n",
      "    pred_class_logits size : (?, 32, 4)\n",
      "    active_class_ids  size : (?, ?)\n",
      ">>> mrcnn_class_loss_graph \n",
      "    target_class_ids  size : (?, 1)\n",
      "    pred_class_logits size : (?, 32, 4)\n",
      "    active_class_ids  size : (?, ?)\n",
      ">>> mrcnn_bbox_loss_graph \n",
      "    target_class_ids  size : (4, ?)\n",
      "    pred_bbox size         : (?, 32, 4, 4)\n",
      "    target_bbox size       : (4, ?, ?)\n",
      ">>> mrcnn_bbox_loss_graph \n",
      "    target_class_ids  size : (?, 1)\n",
      "    pred_bbox size         : (?, 32, 4, 4)\n",
      "    target_bbox size       : (?, 32, 4)\n",
      ">>> mrcnn_mask_loss_graph \n",
      "    target_class_ids size : (4, ?)\n",
      "    target_masks     size : (4, ?, ?, ?)\n",
      "    pred_masks       size : (?, 32, 28, 28, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> mrcnn_mask_loss_graph \n",
      "    target_class_ids size : (?, 1)\n",
      "    target_masks     size : (?, 32, 28, 28)\n",
      "    pred_masks       size : (?, 32, 28, 28, 4)\n",
      ">>> MaskRCNN build complete\n",
      ">>> MaskRCNN initialization complete\n",
      ">>> find_last checkpoint file() \n",
      "    find_last info:   dir_name: E:\\Models\\mrcnn_logs\\shapes20180313T1856\n",
      "    find_last info: checkpoint: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0242.h5\n",
      ">>> load_weights()\n",
      "    load_weights: Loading weights from: E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0242.h5\n",
      "    load_weights: Log directory set to : E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_0242.h5\n",
      ">>> Set_log_dir() -- model dir is  E:\\Models\\mrcnn_logs\n",
      "    set_log_dir: model_path (input) is : E:/Models/mrcnn_logs/shapes20180313T1856/mask_rcnn_shapes_0242.h5  \n",
      "    set_log_dir: self.epoch set to 243  (Next epoch to run)\n",
      "    set_log_dir: tensorboard path: E:\\Models\\mrcnn_logs\\tensorboard\n",
      "    set_log_dir: Checkpoint path set to : E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      ">>> Load weights complete\n",
      "Compile with learing rate; 0.001 Learning Moementum: 0.9 \n",
      "Checkpoint Folder:  E:\\Models\\mrcnn_logs\\shapes20180313T1856\\mask_rcnn_shapes_{epoch:04d}.h5 \n",
      "\n",
      "Selecting layers to train\n",
      "Layer    Layer Name               Layer Type\n",
      "174  fpn_c5p5               (Conv2D)\n",
      "176  fpn_c4p4               (Conv2D)\n",
      "179  fpn_c3p3               (Conv2D)\n",
      "182  fpn_c2p2               (Conv2D)\n",
      "184  fpn_p5                 (Conv2D)\n",
      "185  fpn_p2                 (Conv2D)\n",
      "186  fpn_p3                 (Conv2D)\n",
      "187  fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "      1  rpn_conv_shared        (Conv2D)\n",
      "      2  rpn_class_raw          (Conv2D)\n",
      "      4  rpn_bbox_pred          (Conv2D)\n",
      "199  mrcnn_mask_conv1       (TimeDistributed)\n",
      "200  mrcnn_mask_bn1         (TimeDistributed)\n",
      "202  mrcnn_mask_conv2       (TimeDistributed)\n",
      "204  mrcnn_mask_bn2         (TimeDistributed)\n",
      "205  mrcnn_class_conv1      (TimeDistributed)\n",
      "207  mrcnn_class_bn1        (TimeDistributed)\n",
      "208  mrcnn_mask_conv3       (TimeDistributed)\n",
      "210  mrcnn_mask_bn3         (TimeDistributed)\n",
      "211  mrcnn_class_conv2      (TimeDistributed)\n",
      "213  mrcnn_class_bn2        (TimeDistributed)\n",
      "214  mrcnn_mask_conv4       (TimeDistributed)\n",
      "216  mrcnn_mask_bn4         (TimeDistributed)\n",
      "219  mrcnn_class_logits     (TimeDistributed)\n",
      "220  mrcnn_bbox_fc          (TimeDistributed)\n",
      "221  mrcnn_mask_deconv      (TimeDistributed)\n",
      "227  mrcnn_mask             (TimeDistributed)\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    del model, train_generator, val_generator, mm\n",
    "    gc.collect()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "#model.keras_model.summary(line_length = 120) \n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"last\"  # imagenet, coco, or last\n",
    "if init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    loc=model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    loc= model.load_weights(model.find_last()[1], by_name=True)\n",
    "\n",
    "model.compile_only(learning_rate=config.LEARNING_RATE, layers='heads')\n",
    "KB.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Print some model information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mm = model.keras_model\n",
    "# print('\\n Learning phase values is L ' ,KB.learning_phase())\n",
    "# print('\\n Metrics (_get_deduped_metrics_names():) ') \n",
    "# pp.pprint(mm._get_deduped_metrics_names())\n",
    "# print('\\n Outputs: ') \n",
    "# pp.pprint(mm.outputs)\n",
    "# print('\\n Losses (model.metrics_names): ') \n",
    "# pp.pprint(mm.metrics_names)\n",
    "\n",
    "# model.keras_model.summary(line_length = 120) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:27:58.619142Z",
     "start_time": "2018-04-15T18:27:58.382514Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=model.config.BATCH_SIZE,\n",
    "                                 augment = False)\n",
    "val_generator = data_generator(dataset_val, model.config, shuffle=True, \n",
    "                                batch_size=model.config.BATCH_SIZE,\n",
    "                                augment=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get next shapes from generator and display loaded shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:28:00.449010Z",
     "start_time": "2018-04-15T18:28:00.203371Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generate pyramid anchors \n",
      "    Size of anchor array is : (4092, 4)\n"
     ]
    }
   ],
   "source": [
    "train_batch_x, train_batch_y = next(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display loaded shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:28:03.336695Z",
     "start_time": "2018-04-15T18:28:01.932983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id:  371\n",
      "Image meta [371 128 128   3   0   0 128 128   1   1   1   1]\n",
      "[1 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89684a630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id:  222\n",
      "Image meta [222 128 128   3   0   0 128 128   1   1   1   1]\n",
      "[2 3 1 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89ac86748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id:  259\n",
      "Image meta [259 128 128   3   0   0 128 128   1   1   1   1]\n",
      "[1 3 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89ad4c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image id:  1\n",
      "Image meta [  1 128 128   3   0   0 128 128   1   1   1   1]\n",
      "[2 2 3]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAysAAACoCAYAAADgm3G3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADFZJREFUeJzt3H2sJXddx/HPuqv0CbEEElKSmkiDFYKpxoUa6GQQ46jQFIkRw/KglfiwtrSlhKSJtYiWlo21RbdASUn1j4JgA9qaxlFbptPWgIu2QZRYXSgGbIuGLbS0paV7/WPmttfLPu+9d37nnNcr2dyzc8+Z+zub2XPP+3znnE1LS0sBAAAozfdMvQAAAIB9ESsAAECRxAoAAFAksQIAABRJrAAAAEUSKwAAQJG2TL2AqdVVUyd5bde356/YdnfXt6cd4u3vTPK6rm8fqKvm7CR/0PXtSeP3rk7yia5vb9nPbZ+RpEvyc13fPlhXzQ8n+VCSzUk+1/Xt9rpqTknyZ+O2j3V9e+UB1nJWkrrr2wvGv/9+kp9KsinJuV3f/lNdNb+X5GeTPJJkW9e3/30o9xPgYOqqeXaSV3R9e+OKbdd2ffvWo9xvl+Fx+sGjXCIAM8Zk5ejdluRl4+VXJvlMXTUvHv/+40n+YV83qqvm5CS3JPmhFZsvSXJh17evSPL9ddWcnuScJO9N8pNJttVVc8J+9vf2JDsyhEnqqnlhktO6vn15kjcnubSumh9I8pokpyf5kyRvO6J7DLBvP5rhBZKnHG2oALDYFn6ycijqqrk4QwT8cZK/S9J0ffv18du3JamS3JjkpCQfTPKqumruTfJI17ePjq8KrnR+km8nOTvDJGXZeUn+d7y8JckTSf45yYlJvm/c/kRdNX+b5NJxHxd3ffvqJPck2Z7kzPF6X8oQKSv39Y0k9437OiHJQ4f9j0HRxknh5UmWknw8yYNJzk2yO8nJXd++bOWr1MuXk5wy3u57MxwXZyW5OEMkJ8kbk1yX4bj5ryRnd337nY25V8yQ85JsravmjAyPZW2SN3d9e1pdNb+S4THphCQ3d337rrpq/j7J5zO8gLKr69tz66p5Y5K3ZzjOXtL17QuWdz5Omq/JcJzu6vr2wg28bwBMQKwMfrGumgOd9nVZktuTbE1yyYpQSZI7k1xQV82pSf49w2ld1yT5XJI7kqTr23p/O66r5qnLXd/+z7jt9UmOG0/bem6G08AuSfLJrm+/XVfNryf56PLax9v+9fhEdXlfTyTZM05irk3yjgy/4Pcm+UKSYzM8QWC+nJnkyiR/keQtGZ70bU3y7CSfPcDtTk3ylq5vv1pXzSeTvGjc/qmuby+vq+aPknxwPM4uSvL6JNev151gZr0vQ/yeleTVXd/eX1fN8osmz0ny0xlOaf2XJO/K8DvohiQXJPm3umqOS3Jhhkh+VoYXXVbakeS8rm8/X1fNNXXVvLzr2zvX+T4BMCGngQ1u6Pq2Xv6z+pvjK8h/muTFSf5m1fcezvDv+Kokt47BcXyGEOiS4XzrVX/2G0Z11bwpyW8k2TZuujTD6WWnJDl5/OV8b5J7k9zT9e1XD7CvE5PcnOSqrm8/neG9Kg8necF4+UP7uy0z6/IkZyS5Nclzk3yl69vHxvcmfXEf1980fr0vyZV11VyX4fjYPG6/Z/x6apKLVkxinr8+y2dOPNT17f2rtj2e5CNJdiZ5xort/9r17VKSBzI8dt4/HrMPJPnyqn28MMnO8Tj8iSQ/uB6LZ/HUVbNj/P28Y+q1sLgch/tmsnII6qp5VpK3JvnzDK8AXrHqKncneVOG94Mkw4Tl55NclRx4srLq55yZ4dXw13R9+8i4+RtJHu76dm9dNV/L8F6Wl2b4pf7Mumpe2vXtP+5jX5uT3JTkvV3f3jRufijJt7q+Xaqr5oEMp2MwX345Q5z+Z101d2U4Xo5NckySk8frPJbkeXXVPJ6n3zN1RZImydeTfCZPR8ze8evuJB/v+vb2umqWoxdWW8pw7OxduXF8v9xvdn37orpqnp/kF1bdZtneJCeNHz7yzDx9zC7bneRtXd9+ua6aN2R47IWj1vXtO6deAzgO902sHJorkvxhkk8kuaOumhu7vv2PFd+/LcnPdH27/H6TT2U41/qxw/w570nynSQ3j6eHXZLknUk+WlfN8ulbt2Q49eyXMjwp+Ng4bXl81b5em2ESdGFdNRcm+VLXt79aV83rxk8wS4ZTw5gvdye5oa6aBzMch7dlmPDdl+GV7ST5QIZTb3ZnmNAlw2ljtybZk+SbSZ63ar+XJbm2rpr3ZPgkuTes2z1gln0xw6lex6za/s0ku+uq2TVe/tp+PizkyQzTwTsyvGdldRRflOS6umqOSfKVDI/JAMyxTUtLSwe/FjDzDucjuWEqddWc3/XtVXXVPCfJ7V3f/sjUawJgOiYrAJTkuLpqPpvhUwt/Z+rFADAtkxUAAKBIPg0MAAAoklgBAACKJFYAAIAiFfMG+4t+6zxvnlkgl33gfZsOfq2Nd+yPneM4XCCP3rWzuOPQMbhYSjwGE8fhoinxOHQMLpYDHYMmKwAAQJHECgAAUCSxAgAAFEmsAAAARRIrAABAkcQKAABQJLECAAAUSawAAABFEisAAECRxAoAAFAksQIAABRJrAAAAEUSKwAAQJHECgAAUKQtUy9g0ZzRf2vqJayb26vjp14Ch+jXfve3p17Cuvnwu6+eegkAwBoxWQEAAIokVgAAgCKJFQAAoEhiBQAAKJJYAQAAiiRWAACAIokVAACgSGIFAAAoklgBAACKJFYAAIAiiRUAAKBIYgUAACiSWAEAAIokVgAAgCKJFQAAoEhiBQAAKJJYAQAAiiRWAACAIokVAACgSGIFAAAoklgBAACKJFYAAIAiiRUAAKBIYgUAACiSWAEAAIokVgAAgCKJFQAAoEhiBQAAKNKWqRcwleb0s6b5waev3a4e2fGRtdsZk9iza+fUSzhq77jpC1MvAQCYUyYrAABAkcQKAABQJLECAAAUSawAAABFEisAAECRxAoAAFAksQIAABRJrAAAAEUSKwAAQJHECgAAUCSxAgAAFEmsAAAARRIrAABAkcQK3+Ulr/zLqZcA+fC7r556CSy4Pbt2Tr0EgIUnVvh/lkNFsDCl5VARLExlOVQEC8C0xAoAAFAkscJTVk9TTFeYwuppiukKG231NMV0BWA6YgUAACiSWCHJ/qcopitspP1NUUxX2Cj7m6KYrgBMQ6wAAABFEiscdHpiusJGONj0xHSF9Xaw6YnpCsDGEysLTohQAiHC1IQIQJnECodE1FACUcPURA3AxhIrC+xwA0SwsB4ON0AEC2vtcANEsABsHLECAAAUSawsqCOdkpiusJaOdEpiusJaOdIpiekKwMbYMvUCOHK3V8cf0e22b75+kp/LfJoqHAQLR0twAJTPZIXDdrSxAzAPxA7A+hMrC0ZoAAgNgFkhVgAAgCKJlQViqgJgqgIwS8TKgljrUNm++XrxA8yctQ6VPbt2ih+AdSRWFoCoADBRAZhFYoWjIoQAhBDAehErc05MAIgJgFklVjhqgghAEAGsB7Eyx0QEgIgAmGViZU5tdKgII6BEGx0qwghgbYkVAACgSGJlDk015TBdAUoy1ZTDdAVg7YgV1pRgARAsAGtFrMwZsQAgFgDmhVhhzQkmAMEEsBbEyhwRCQAiAWCeiJU5UVqolLYeYDGUFiqlrQdg1ogVAACgSGJlDpQ6xSh1XcB8KnWKUeq6AGaBWAEAAIq0ZeoFTKX99F9NvYQ1Ufr0Yvvm6/P+J7dNvYxinbj1nKmXAHOh9OnFnl07/X8HOAImK6y70oMKYCOUHlQAJRIrM0wEAIgAgHkmVtgQwgpAWAEcLrECAAAUSazMqFmcVMzimoGyzeKkYhbXDDAVsQIAABRJrMygWZ5QzPLagbLM8oRiltcOsJHEyozxZB/Ak32ARSFW2HCCC0BwARwKsTJDPMkH8CQfYJGIFQAAoEhiZUaYqgCYqgAsGrECAAAUacvUC+DQvP/JbVMvYU1dNvUCgJl04tZzpl7Cmnr0LpMigAMxWQEAAIokVgAAgCKJFQAAoEhiBQAAKJJYAQAAiiRWAACAIokVAACgSGIFAAAoklgBAACKJFYAAIAiiRUAAKBIYgUAACiSWAEAAIokVgAAgCKJFQAAoEiblpaWpl4DAADAdzFZAQAAiiRWAACAIokVAACgSGIFAAAoklgBAACKJFYAAIAiiRUAAKBIYgUAACiSWAEAAIokVgAAgCKJFQAAoEhiBQAAKJJYAQAAiiRWAACAIokVAACgSGIFAAAoklgBAACKJFYAAIAiiRUAAKBIYgUAACiSWAEAAIokVgAAgCKJFQAAoEhiBQAAKNL/AciTC9GvkdSAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89afcb6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_batch_x, train_batch_y = next(train_generator)\n",
    "imgmeta_idx = model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "for img_idx in range(config.BATCH_SIZE):\n",
    "    image_id = img_meta[img_idx,0]\n",
    "    print('Image id: ',image_id)\n",
    "    print('Image meta', img_meta[img_idx])\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print(class_ids)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from   mrcnn.utils            import parse_image_meta_graph\n",
    "# a,b,c,d = parse_image_meta_graph(img_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Data thru model using get_layer_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:28:05.502458Z",
     "start_time": "2018-04-15T18:28:05.270843Z"
    }
   },
   "outputs": [],
   "source": [
    "from mrcnn.callbacks import get_layer_output_1,get_layer_output_2\n",
    "np.set_printoptions(linewidth=100,precision=4)\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T18:28:10.920874Z",
     "start_time": "2018-04-15T18:28:06.921233Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* Inputs */\n",
      "Input 0:  (input_image             ) \t  Input shape: (4, 128, 128, 3)\n",
      "Input 1:  (input_image_meta        ) \t  Input shape: (4, 12)\n",
      "Input 2:  (input_rpn_match         ) \t  Input shape: (4, 4092, 1)\n",
      "Input 3:  (input_rpn_bbox          ) \t  Input shape: (4, 256, 4)\n",
      "Input 4:  (input_gt_class_ids      ) \t  Input shape: (4, 100)\n",
      "Input 5:  (input_gt_boxes          ) \t  Input shape: (4, 100, 4)\n",
      "Input 6:  (input_gt_masks          ) \t  Input shape: (4, 56, 56, 100)\n",
      "num rois : 32\n",
      "===  Build Gaussian np for detected rois =========================\n",
      "   input_tensor shape is  (4, 4, 32, 8)\n",
      "   num of bboxes per class is :  32\n",
      " COVARIANCE SHAPE: (32, 2)\n",
      "====> Img:  0\n",
      " ===> mns shape  (32, 2) coar.shape  (32, 2)\n",
      "  pdf_arr.shape :  (128, 128, 30)\n",
      "   *** img:  0  cls: 0   [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "   *** img:  0  cls: 1   [21 22 23 24 25 26 27]\n",
      "   *** img:  0  cls: 2   [28 29]\n",
      "   *** img:  0  cls: 3   []\n",
      "====> Img:  1\n",
      " ===> mns shape  (32, 2) coar.shape  (32, 2)\n",
      "  pdf_arr.shape :  (128, 128, 30)\n",
      "   *** img:  1  cls: 0   [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      "   *** img:  1  cls: 1   [14 15 16 17 18]\n",
      "   *** img:  1  cls: 2   [19 20 21 22]\n",
      "   *** img:  1  cls: 3   [23 24 25 26 27 28 29]\n",
      "====> Img:  2\n",
      " ===> mns shape  (32, 2) coar.shape  (32, 2)\n",
      "  pdf_arr.shape :  (128, 128, 30)\n",
      "   *** img:  2  cls: 0   [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "   *** img:  2  cls: 1   []\n",
      "   *** img:  2  cls: 2   [20 21 22 23 24 25]\n",
      "   *** img:  2  cls: 3   [26 27 28 29]\n",
      "====> Img:  3\n",
      " ===> mns shape  (32, 2) coar.shape  (32, 2)\n",
      "  pdf_arr.shape :  (128, 128, 30)\n",
      "   *** img:  3  cls: 0   [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "   *** img:  3  cls: 1   [18 19 20 21 22 23 24 25 26 27]\n",
      "   *** img:  3  cls: 2   []\n",
      "   *** img:  3  cls: 3   [28 29]\n",
      "Zout shape: (4, 4, 128, 128)\n",
      "covar_arrc :  (4, 32, 2)   means_arr shae (4, 32, 2)\n",
      "pdf_arr :  float64\n",
      "   pred scatter :  (128, 128, 30)    pcn gaussian :  (4, 4, 128, 128)\n",
      "   pred means  :  (4, 32, 2)     covar :  (4, 32, 2)\n",
      "==-  Build Gaussian np for ground_truth ==========================\n",
      "   input_tensor shape is  (4, 4, 100, 8)\n",
      "   num of bboxes per class is :  100\n",
      " COVARIANCE SHAPE: (100, 2)\n",
      "====> Img:  0\n",
      " ===> mns shape  (100, 2) coar.shape  (100, 2)\n",
      "  pdf_arr.shape :  (128, 128, 2)\n",
      "   *** img:  0  cls: 1   0\n",
      "   *** img:  0  cls: 2   1\n",
      "   *** img:  0  cls: 3   []\n",
      "====> Img:  1\n",
      " ===> mns shape  (100, 2) coar.shape  (100, 2)\n",
      "  pdf_arr.shape :  (128, 128, 4)\n",
      "   *** img:  1  cls: 1   0\n",
      "   *** img:  1  cls: 2   [1 2]\n",
      "   *** img:  1  cls: 3   3\n",
      "====> Img:  2\n",
      " ===> mns shape  (100, 2) coar.shape  (100, 2)\n",
      "  pdf_arr.shape :  (128, 128, 3)\n",
      "   *** img:  2  cls: 1   [0 1]\n",
      "   *** img:  2  cls: 2   []\n",
      "   *** img:  2  cls: 3   2\n",
      "====> Img:  3\n",
      " ===> mns shape  (100, 2) coar.shape  (100, 2)\n",
      "  pdf_arr.shape :  (128, 128, 3)\n",
      "   *** img:  3  cls: 1   []\n",
      "   *** img:  3  cls: 2   [0 1]\n",
      "   *** img:  3  cls: 3   2\n",
      "Zout shape: (4, 4, 128, 128)\n",
      "covar_arrc :  (4, 100, 2)   means_arr shae (4, 100, 2)\n",
      "pdf_arr :  float64\n",
      "   gt scatter :  (128, 128, 3)    gt gaussian :  (4, 4, 128, 128)\n",
      "   gt means   :  (4, 100, 2)    gt covar :  (4, 100, 2)\n",
      "\n",
      "/* Outputs */\n",
      "Output 0: (output_rois             ) \t  Output shape: (4, 32, 4)\n",
      "Output 1: (proposal_targets        ) \t  Output shape: (4, 32)\n",
      "Output 2: (proposal_targets        ) \t  Output shape: (4, 32, 4)\n",
      "Output 3: (proposal_targets        ) \t  Output shape: (4, 32, 28, 28)\n",
      "Output 4: (cntxt_layer             ) \t  Output shape: (4, 4, 128, 128)\n",
      "Output 5: (cntxt_layer             ) \t  Output shape: (4, 32, 2)\n",
      "Output 6: (cntxt_layer             ) \t  Output shape: (4, 32, 2)\n",
      "Output 7: (cntxt_layer             ) \t  Output shape: (4, 4, 128, 128)\n",
      "Output 8: (cntxt_layer             ) \t  Output shape: (4, 100, 2)\n",
      "Output 9: (cntxt_layer             ) \t  Output shape: (4, 100, 2)\n",
      "Output 10: (cntxt_layer             ) \t  Output shape: (4, 4, 32, 8)\n",
      "Output 11: (cntxt_layer             ) \t  Output shape: (4, 4)\n",
      "Output 12: (cntxt_layer             ) \t  Output shape: (4, 4, 100, 8)\n",
      "Output 13: (cntxt_layer             ) \t  Output shape: (4, 4)\n",
      "Output 14: (cntxt_layer_2           ) \t  Output shape: (4, 4, 128, 128)\n",
      "Output 15: (cntxt_layer_2           ) \t  Output shape: (4, 32, 2)\n",
      "Output 16: (cntxt_layer_2           ) \t  Output shape: (4, 32, 2)\n",
      "Output 17: (cntxt_layer_2           ) \t  Output shape: (4, 4, 32, 8)\n",
      "Output 18: (cntxt_layer_2           ) \t  Output shape: (4, 4)\n",
      "Output 19: (cntxt_layer_2           ) \t  Output shape: (4, 4, 100, 8)\n",
      "Output 20: (cntxt_layer_2           ) \t  Output shape: (4, 4)\n",
      "Output 21: (rpn_class_logits        ) \t  Output shape: (4, 4092, 2)\n",
      "Output 22: (proposal_rois           ) \t  Output shape: (4, 2000, 4)\n",
      "Output 23: (rpn_class               ) \t  Output shape: (4, 4092, 2)\n",
      "Output 24: (rpn_bbox                ) \t  Output shape: (4, 4092, 4)\n",
      "Output 25: (mrcnn_class_logits      ) \t  Output shape: (4, 32, 4)\n",
      "Output 26: (mrcnn_class             ) \t  Output shape: (4, 32, 4)\n",
      "Output 27: (mrcnn_bbox              ) \t  Output shape: (4, 32, 4, 4)\n",
      "Output 28: (mrcnn_mask              ) \t  Output shape: (4, 32, 28, 28, 4)\n",
      "Output 29: (rpn_class_loss          ) \t  Output shape: ()\n",
      "Output 30: (rpn_bbox_loss           ) \t  Output shape: ()\n",
      "Output 31: (rpn_bbox_loss_old       ) \t  Output shape: ()\n",
      "Output 32: (mrcnn_class_loss        ) \t  Output shape: (1, 1)\n",
      "Output 33: (mrcnn_bbox_loss         ) \t  Output shape: (1, 1)\n",
      "Output 34: (mrcnn_mask_loss         ) \t  Output shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "layers_out = get_layer_output_2(model.keras_model, train_batch_x, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control pred_tensor / pred_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    train_batch_x, train_batch_y = next(train_generator)\n",
    "\n",
    "    layers_out = get_layer_output_2(model.keras_model, train_batch_x, 1, verbose = False)\n",
    "\n",
    "    pt   = layers_out[8]   # pred_tensor\n",
    "    pcc  = layers_out[9]   # pred_cls_cnt\n",
    "\n",
    "    pt2  = layers_out[12]  # pred_TNESOR_2\n",
    "    pcc2 = layers_out[13]  # pred_cls_cnt_2\n",
    "\n",
    "    # print( pt2.shape, pcc2.shape)\n",
    "    # print( pt.shape, pcc.shape)\n",
    "    # print(pc2)\n",
    "\n",
    "    for img in range(config.BATCH_SIZE):\n",
    "        for cls in range(4):\n",
    "        #     print(pt2[img][cls])\n",
    "        #     print(pt[img][cls])\n",
    "            equal = np.all(pt2[img][cls]== pt[img][cls,:,:7], axis = -1)\n",
    "\n",
    "            if (~equal.all()):\n",
    "                print(' Iteration', i , 'Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "                print(equal)\n",
    "                print('\\n -- using numpy \\n',pt[img][cls,~equal,:-1])\n",
    "                print('\\n -- using tensorflow \\n',pt2[img][cls,~equal])\n",
    "                print()\n",
    "    #             print('\\n -- using numpy \\n',pt[img][cls])            \n",
    "    #             print('\\n -- using tensorflow \\n',pt2[img][cls])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control means, covar / means2, covar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:06:35.104017Z",
     "start_time": "2018-04-15T15:06:34.838337Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mns  = layers_out[5]   # pred_tensor\n",
    "cov  = layers_out[6]   # pred_cls_cnt\n",
    "\n",
    "mns2 = layers_out[15]  # pred_TNESOR_2\n",
    "cov2 = layers_out[16]  # pred_cls_cnt_2\n",
    "\n",
    "\n",
    "print( mns.shape, mns2.shape)\n",
    "# print( pt.shape, pcc.shape)\n",
    "# print(pc2)\n",
    "\n",
    "for img in range(config.BATCH_SIZE):\n",
    "    for cls in range(4):\n",
    "        equal = np.all(mns[img][cls]== mns2[img][cls], axis = -1)\n",
    "        print('Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "#         print('mns  :')\n",
    "#         print(mns[img])\n",
    "#         print('mns2 :')\n",
    "#         print(mns2[img])\n",
    "\n",
    "        if (~equal.all()):\n",
    "            print('Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "            print(equal)\n",
    "            print('\\n -- numpy      \\n', mns[img][cls,~equal,:-1])\n",
    "            print('\\n -- tensorflow \\n', mns2[img][cls,~equal])\n",
    "            print()\n",
    "#             print('\\n -- using numpy \\n',pt[img][cls])            \n",
    "#             print('\\n -- using tensorflow \\n',pt2[img][cls])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control pred_tesnor / pred_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:06:50.640259Z",
     "start_time": "2018-04-15T15:06:50.384581Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pt   = layers_out[10]   # pred_tensor\n",
    "pt2  = layers_out[17]   # pred_cls_cnt\n",
    " \n",
    "# print( pt2.shape, pcc2.shape)\n",
    "print( pt.shape, pt2.shape)\n",
    "# print(pc2)\n",
    "\n",
    "for img in range(config.BATCH_SIZE):\n",
    "    for cls in range(4):\n",
    "        equal = np.all(pt[img][cls,:,1:7] == pt2[img][cls,:,1:7], axis = -1)\n",
    "        print('Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "#         print( pt[img,cls])\n",
    "#         print()\n",
    "#         print(pt2[img,cls])\n",
    "\n",
    "\n",
    "\n",
    "        if (~equal.all()):\n",
    "#             print('Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "            print(equal)\n",
    "            print('\\n -- using numpy (pt) \\n',pt[img][cls,~equal,:-1])\n",
    "            print('\\n -- using tensorflow (pt2) \\n',pt2[img][cls,~equal])\n",
    "            print()\n",
    "#             print('\\n -- using numpy \\n',pt[img][cls])            \n",
    "#             print('\\n -- using tensorflow \\n',pt2[img][cls])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control pred_cls_cnt / pred_cls_cnt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:06:58.113148Z",
     "start_time": "2018-04-15T15:06:57.869495Z"
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "pt  = layers_out[11]   # pred_cls_cnt\n",
    "pt2 = layers_out[18]  # pred_cls_cnt_2\n",
    "\n",
    "\n",
    "# print( pt2.shape, pcc2.shape)\n",
    "# print( pt.shape, pcc.shape)\n",
    "# print(pc2)\n",
    "\n",
    "for img in range(config.BATCH_SIZE):\n",
    "    print('  pt2  ', pt2[img])\n",
    "    print('  pt   ', pt[img])\n",
    "    equal = np.all(pt2[img]== pt[img], axis = -1)\n",
    "\n",
    "    if (~equal.all()):\n",
    "        print('Image ',img, ' ALL EQUAL: ',equal.all())\n",
    "        print(equal)\n",
    "        print('\\n -- using numpy \\n',pt[img][~equal])\n",
    "        print('\\n -- using tensorflow \\n',pt2[img][~equal])\n",
    "        print()\n",
    "#             print('\\n -- using numpy \\n',pt[img][cls])            \n",
    "#             print('\\n -- using tensorflow \\n',pt2[img][cls])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  pred_gaussian / pred_gaussian2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:07:13.081003Z",
     "start_time": "2018-04-15T15:07:12.526493Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt   = layers_out[4]   # pred_gaussian \n",
    "pt2  = layers_out[14]  # pred_gaussian_2\n",
    "\n",
    "print( ' pt shape ', pt.shape, ' pt2.shape ', pt2.shape)\n",
    "\n",
    "for img in range(config.BATCH_SIZE):\n",
    "    print(' from np ')\n",
    "    print(pt[img])\n",
    "    print(' from tensorflow')\n",
    "    print(pt2[img])\n",
    "\n",
    "    for cls in range(4):\n",
    "\n",
    "        equal = np.all(pt2[img, cls]== pt[img, cls], axis = -1)\n",
    "        \n",
    "# #         print('\\n -- using numpy \\n',pt[img][cls])            \n",
    "# #         print('\\n -- using tensorflow \\n',pt2[img][cls])\n",
    "        print( 'Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "        \n",
    "        \n",
    "#         if (~equal.all()):\n",
    "#             print( 'Image ',img,' Class ',cls, ' ALL EQUAL: ',equal.all())\n",
    "#             print(equal)\n",
    "#             print('\\n -- using numpy \\n',pt[img][cls,~equal,:-1])\n",
    "#             print('\\n -- using tensorflow \\n',pt2[img][cls,~equal])\n",
    "#             print()\n",
    "# #             print('\\n -- using numpy \\n',pt[img][cls])            \n",
    "# #             print('\\n -- using tensorflow \\n',pt2[img][cls])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rois = layers_out[0]\n",
    "out_rois[0] * np.array([128,128,128,128])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:09:14.721145Z",
     "start_time": "2018-04-15T15:09:13.976134Z"
    }
   },
   "outputs": [],
   "source": [
    "imgmeta_idx = model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta    = train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot mask in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=99999, linewidth=2000)\n",
    "# print(np.array2string(mask[...,0],max_line_width=2000,separator=''))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outmask0 = layers_out[14][0,:,:,:,1] ##  mrcnn_mask\n",
    "np.max(outmask0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Predicition Probability Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T16:45:41.229179Z",
     "start_time": "2018-04-15T16:45:33.040392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mrcnn.visualize import plot_gaussian\n",
    "Zout = layers_out[4]\n",
    "Zout2 = layers_out[14]\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout[img,cls], title = ttl)\n",
    "        plot_gaussian(Zout2[img,cls], title = ttl)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Ground Truth Probability tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-15T15:11:10.262088Z",
     "start_time": "2018-04-15T15:11:06.423871Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# print(layers_out[i].shape)      #[0,0,0:20, 0:20]\n",
    "Zout = layers_out[14]\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout[img,cls], title = ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display predicted bounding boxes - calculate center and width/height of bboxes displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.utils import trim_zeros\n",
    "np.set_printoptions( edgeitems=32, suppress=True)\n",
    "pred_bb = layers_out[3]\n",
    "print(pred_bb.shape)\n",
    "x0 = [ trim_zeros((pred_bb[0,i,:,:])) for i in range(4)]\n",
    "ps0 = np.concatenate( x0, axis=0 )\n",
    "\n",
    "x1 = [ trim_zeros((pred_bb[1,i,:,:])) for i in range(4)]\n",
    "ps1 = np.concatenate( x1, axis=0 )\n",
    "# print(np.concatenate( x1, axis=0 ))\n",
    "print(ps0)\n",
    "print(ps0.shape)\n",
    "width  = ps0[:,5] - ps0[:,3]\n",
    "height = ps0[:,4] - ps0[:,2]\n",
    "cx     = ps0[:,3] + ( width  / 2.0)\n",
    "cy     = ps0[:,2] + ( height / 2.0)\n",
    "means0  = np.stack((cx,cy,width, height),axis = -1)\n",
    "print(means0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output RoIs (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_rois = layers_out[0]\n",
    "output_rois[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display ground truth bboxes from Shapes database (using load_image_gt)\n",
    "\n",
    "Here we are displaying the ground truth bounding boxes as provided by the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "p_original_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "# print(p_gt_class_id.shape, p_gt_bbox.shape, p_gt_mask.shape)\n",
    "print(p_gt_bbox)\n",
    "visualize.draw_boxes(p_original_image, p_gt_bbox)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "p_original_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "# print(p_gt_class_id.shape, p_gt_bbox.shape, p_gt_mask.shape)\n",
    "print(p_gt_bbox)\n",
    "visualize.draw_boxes(p_original_image, p_gt_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display bboxes from Ground Truth Info - Input info Passed to Network \n",
    "\n",
    "layers_out[5]  gt_tensor is based on input_gt_class_ids and input_normlzd_gt_boxes\n",
    "\n",
    "Display the Ground Truth bounding boxes from the tensor we've constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=120, precision=5)\n",
    "gt_bboxes = layers_out[5]  \n",
    "print(layers_out[5].shape)\n",
    "print(' gt_cls_cnt')\n",
    "print(layers_out[6])\n",
    "print(layers_out[5][1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gt_bboxes)\n",
    "# visualize.display_instances(p_original_image, p_gt_bbox, p_gt_mask, p_gt_class_id, \n",
    "#                             dataset_train.class_names, figsize=(8, 8))\n",
    "# pp.pprint(gt_bboxes)\n",
    "img = 0\n",
    "image_id = img_meta[img,0]\n",
    "print('Image id: ',image_id)\n",
    "p_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "gt_bboxes = layers_out[5]\n",
    "print(gt_bboxes.shape)\n",
    "print(gt_bboxes[0,1,0:1,2:6])\n",
    "print(gt_bboxes[0,2,0:2,2:6])\n",
    "gt_bb = np.vstack((gt_bboxes[0,1,0:1,2:6],gt_bboxes[0,2,0:2,2:6],gt_bboxes[0,3,0:2,2:6]))\n",
    "gt_bb.shape\n",
    "visualize.draw_boxes(p_image, gt_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display RoI proposals generated\n",
    "\n",
    "Display bounding boxes from tensor of proposals produced by the network \n",
    "Square: 1 , Circle:2 , Triangle -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = 0\n",
    "cls = 3  # <==== Class to dispaly\n",
    "image_id = img_meta[img,0]\n",
    "print('Image id: ',image_id)\n",
    "p_image, p_image_meta, p_gt_class_id, p_gt_bbox, p_gt_mask =  \\\n",
    "            load_image_gt(dataset_train, config, image_id, augment=False, use_mini_mask=True)\n",
    "\n",
    "pred_tensor = layers_out[3]\n",
    "caps = [str(cls)+'-'+str(x) for x in pred_tensor[img,cls,:,0].astype('int16').tolist() ]\n",
    "print(caps)\n",
    "# print(pc_tensor.pred_tensor[1,3,:])\n",
    "# print(pc_tensor.pred_tensor[1,3,:,2:6])\n",
    "visualize.draw_boxes(p_image, pred_tensor[img,cls,:,2:6], captions = caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class:\n",
    "- determine the center of each bounding box.\n",
    "- center a 2d gaussian distribution with the mean = center of bounding box and sigma = height/width\n",
    "- place dist on mesh grid\n",
    "- normalize\n",
    "- draw heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "from mrcnn.pc_layer import PCTensor\n",
    "pc_tensor = PCTensor(model)\n",
    "pc_tensor.build_predictions(sample_x)\n",
    "print(pc_tensor.pred_stacked)    # list of tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.visualize import plot_gaussian\n",
    "num_images = config.IMAGES_PER_GPU\n",
    "num_classes = config.NUM_CLASSES\n",
    "for img in range(num_images):\n",
    "    for cls in range(num_classes):\n",
    "        ttl = 'image :  {} class: {} '.format(img,cls)\n",
    "        plot_gaussian(Zout1[img,cls], title = ttl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# img = 0\n",
    "# cls = 0\n",
    "# _cnt = pc_tensor.pred_cls_cnt[img,cls]\n",
    "# print(_cnt)\n",
    "# for box in range(_cnt):\n",
    "\n",
    "#     mns = means[img,cls, 0 : _cnt]\n",
    "#     print('img: ',img, 'class: ', cls, 'class count: ',_cnt, 'shape of mns :',mns.shape)\n",
    "#     # print('** bbox is : ' ,self.pred_tensor[img,cls,box])\n",
    "#     # print('    center is ({:4f},{:4f})  width is {:4f} height is {:4f} '\\\n",
    "#         # .format(mns[0],mns[1],width[img,cls,box],height[img,cls,box]))            \n",
    "#     # fn = lambda x: multivariate_normal(x, [[12,0.0] , [0.0,19]])\n",
    "#     # rv = tf.map_fn(fn, \n",
    "#     rv = np.apply_along_axis(multivariate_normal, 1, mns, [[12,0.0] , [0.0,19]])\n",
    "#     print('rv :',rv.shape, rv)\n",
    "#     _zo = rv.pdf(pos[img,cls])\n",
    "#     print('zo :',_zo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tf.contrib.distributions\n",
    "k_sess = KB.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp1 = tf.fill([1,1,32], 12.0)\n",
    "# pp2 = tf.fill([1,1,32], 19.0)\n",
    "# pp  = tf.cast(tf.stack((pp1,pp2),axis=-1), dtype=tf.float64)\n",
    "# tf.cast([12.0, 19.00], dtype=tf.float64)\n",
    "# pp1.eval(session = k_sess)\n",
    "\n",
    "# mvn = tfd.MultivariateNormalDiag(means[0,0,0,:],scale_diag=p1)\n",
    "# mvn = tfd.MultivariateNormalDiag(means[0,0,0,:],scale_diag=p1)\n",
    "\n",
    "# with k_sess.as_default():\n",
    "#     print(mvn.mean())\n",
    "#     print(mvn.batch_shape)\n",
    "#     print(mvn.event_shape)\n",
    "#     print(pos[0,0,:,0,0,:].shape)\n",
    "#     rr = mvn.prob(pos[0,0,:,0,0,:])\n",
    "#     print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# from mrcnn.visualize import plot_gaussian\n",
    "# for i in range(0,config.IMAGES_PER_GPU):\n",
    "#     for j in range(0,config.NUM_CLASSES):\n",
    "#         ttl = 'image : {} class: {}'.format(i,j)\n",
    "#         plot_gaussian(Zout[i,j] , title = ttl )\n",
    "# # plot_gaussian(Zout[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zout = np.zeros((num_classes, 128,128))\n",
    "for i in range(1,config.NUM_CLASSES):\n",
    "    print('class: ',i)\n",
    "    for j in range(gt_cls_cnt[i]):\n",
    "        Zout[i] = bbox_gaussian(gt_cpb[i,j], Zout[i])\n",
    "print(Zout.shape)\n",
    " \n",
    "# plot_gaussian(Zout[1])\n",
    "# plot_gaussian(Zout[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "Fine tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=211,\n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T22:48:21.160813Z",
     "start_time": "2018-04-14T22:48:04.860Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# del history\n",
    "try :\n",
    "    del model\n",
    "    gc.collect()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)\n",
    "#model.keras_model.summary(line_length = 120)\n",
    "# print(model.find_last())\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"last\"  # imagenet, coco, or last\n",
    "if init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    loc=model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    loc= model.load_weights(model.find_last()[1], by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training head using  Keras.model.fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run =2, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Training heads using train_in_batches ()\n",
    "\n",
    "We need to use this method for the time being as the fit generator does not have provide EASY access to the output in Keras call backs. By training in batches, we pass a batch through the network, pick up the generated RoI detections and bounding boxes and generate our semantic / gaussian tensors ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_in_batches(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs_to_run = 2,\n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate one training iteration - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mrcnn.datagen import data_generator, load_image_gt\n",
    "np.set_printoptions(linewidth=100)\n",
    "learning_rate=model.config.LEARNING_RATE\n",
    "epochs_to_run = 2\n",
    "layers='heads'\n",
    "batch_size = 0\n",
    "steps_per_epoch = 0\n",
    "# assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "# Pre-defined layer regular expressions\n",
    "layer_regex = {\n",
    "    # all layers but the backbone\n",
    "    \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # From a specific Resnet stage and up\n",
    "    \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "    # All layers\n",
    "    \"all\": \".*\",\n",
    "}\n",
    "\n",
    "if layers in layer_regex.keys():\n",
    "    layers = layer_regex[layers]\n",
    "if batch_size == 0 :\n",
    "    batch_size = model.config.BATCH_SIZE            \n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = model.config.STEPS_PER_EPOCH\n",
    "\n",
    "# Data generators\n",
    "train_generator = data_generator(dataset_train, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size)\n",
    "val_generator   = data_generator(dataset_val, model.config, shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 augment=False)\n",
    "\n",
    "# Train\n",
    "log(\"Last epoch completed : {} \".format(model.epoch))\n",
    "log(\"Starting from epoch {} for {} epochs. LR={}\".format(model.epoch, epochs_to_run, learning_rate))\n",
    "log(\"Steps per epoch:    {} \".format(steps_per_epoch))\n",
    "log(\"Batchsize      :    {} \".format(batch_size))\n",
    "log(\"Checkpoint Folder:  {} \".format(model.checkpoint_path))\n",
    "epochs = model.epoch + epochs_to_run\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "if not gfile.IsDirectory(model.log_dir):\n",
    "    log('Creating checkpoint folder')\n",
    "    gfile.MakeDirs(model.log_dir)\n",
    "else:\n",
    "    log('Checkpoint folder already exists')\n",
    "\n",
    "model.set_trainable(layers)            \n",
    "model.compile(learning_rate, model.config.LEARNING_MOMENTUM)        \n",
    "\n",
    "out_labels = model.keras_model._get_deduped_metrics_names()\n",
    "callback_metrics = out_labels + ['val_' + n for n in out_labels]\n",
    "\n",
    "progbar = keras.callbacks.ProgbarLogger(count_mode='steps')\n",
    "progbar.set_model(model.keras_model)\n",
    "progbar.set_params({\n",
    "    'epochs': epochs,\n",
    "    'steps': steps_per_epoch,\n",
    "    'verbose': 1,\n",
    "    'do_validation': False,\n",
    "    'metrics': callback_metrics,\n",
    "})\n",
    "\n",
    "progbar.set_model(model.keras_model) \n",
    "\n",
    "chkpoint = keras.callbacks.ModelCheckpoint(model.checkpoint_path, \n",
    "                                           monitor='loss', verbose=1, save_best_only = True, save_weights_only=True)\n",
    "chkpoint.set_model(model.keras_model)\n",
    "\n",
    "progbar.on_train_begin()\n",
    "epoch_idx = model.epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate one training iteration - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T14:03:38.566464Z",
     "start_time": "2018-04-13T14:03:38.322749Z"
    }
   },
   "outputs": [],
   "source": [
    "if epoch_idx >= epochs:\n",
    "    print('Final epoch {} has already completed - Training will not proceed'.format(epochs))\n",
    "\n",
    "# while epoch_idx < epochs :\n",
    "progbar.on_epoch_begin(epoch_idx)\n",
    "steps_index = 0\n",
    "# for steps_index in range(steps_per_epoch):\n",
    "\n",
    "batch_logs = {}\n",
    "print(' self.epoch {}   epochs {}  step {} '.format(model.epoch, epochs, steps_index))\n",
    "batch_logs['batch'] = steps_index\n",
    "batch_logs['size']  = batch_size\n",
    "progbar.on_batch_begin(steps_index, batch_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate one training iteration - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgmeta_idx= model.keras_model.input_names.index('input_image_meta')\n",
    "img_meta  =  train_batch_x[imgmeta_idx]\n",
    "\n",
    "image_id = img_meta[0,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "image_id = img_meta[1,0]\n",
    "print('Image id: ',image_id)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)\n",
    "\n",
    "outs = model.keras_model.train_on_batch(train_batch_x, train_batch_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [TF_gpu]",
   "language": "python",
   "name": "Python [TF_gpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
